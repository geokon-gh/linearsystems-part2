#+TITLE: Least Squares
#+DESCRIPTION: Some linear algebra in Clojure


#+HTML_DOCTYPE: html5
#+HTML_LINK_UP: ..
#+HTML_LINK_HOME: ..
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../web/worg.css" />
#+HTML_HEAD_EXTRA: <link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
#+HTML_MATHJAX: path: "../MathJax/MathJax.js?config=TeX-AMS_CHTML"
#+OPTIONS: html-style:nil
#+OPTIONS: num:nil
#+OPTIONS: html-postamble:nil
#+OPTIONS: html-scripts:nil

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/leastsquares.clj
  (ns morelinear.leastsquares
    (:require [clojure.core.matrix :refer :all]
	      [clojure.core.matrix.linear :as linear]
	      [morelinear.gauss :as gauss]))
  (set-current-implementation :vectorz) 
#+END_SRC

* The A^{T}Ax=A^{T}b form

Now that we have many ways to solve square linear systems we need to extend *Ax=b* to the overdefined case where we have more linear equations than parameters. This situation will come up constantly, generally in situations where you only have a few inputs you want to solve for, but you have a lot of redundant measurements. 

\begin{equation}
\begin{bmatrix}
a_11 & a_12\\
a_21 & a_22\\
a_31 & a_32\\
a_41 & a_42\\
...\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\end{bmatrix}
=
\begin{bmatrix}
y_1\\
y_2\\
\\
\end{bmatrix}
\end{equation}


If the measurements were ideal then the rows of *A* will not be independent and you will be able to find an explicit solution though Gaussian Elimination. But in the general case (with the addition of noise and rounding errors) no explicit solution will exist for *A_{skinny}x=b* . You could try to find a set of independent equations and throw away the extra equation to try to make a square matrix, but this is throwing information away.

Since there is no *x* for which *A_{skinny}x=b* holds true we instead solve for a "close" system *A_{skinny}x=b_{close}* which does have a solution. In other words we want to find an *x* that will give us a *b_{close}* which is as close as possible to *b*. When we say "close" what we mean mathematically is that we want to minimize the sum of the difference between *b_{close}* and *b* - ie. *sum_of_all_values(b_{close}-b)*.

The problem is that sums don't express very easily in matrix form. Fortunately we do have a mechanism which is really close - the *inner product*. The inner product of a vector *x* with itself - *x^{T}x* - is the sum of the squares of the values of *x*. While this is not equivalent, it actually does not change the solution b/c the minimum stays the minimum. So instead of  *sum_of_all_values(b_{close}-b)*. we can work with *(b_{close}-b)^{T}(b_{close}-b)* and guarantee the same solution

If we plug in our equation for *b_{close}* for we get *(A_{skinny}x-b)^{T}(A_{skinny}x-b)*. 

\begin{equation}
(A_{skinny}x-b)^{T}(A_{skinny}x-b) \\
((A_{skinny}x)^{T}-b^{T})(A_{skinny}x-b) \\
(x^{T}A_{skinny}^{T}-b^{T})(A_{skinny}x-b) \\
x^{T}A_{skinny}^{T}A_{skinny}x
-x^{T}A_{skinny}^{T}b
-b^{T}A_{skinny}x
+b^2
\end{equation}


As we learn in calculus, minimizing a function is done by take its derivative with respect to the parameter we are changing (here that's =x=), setting it equal to zero and then solving for that parameter (b/c the minimum point has zero slope). What /page 226-227/ shows is that the derivative of our difference equation give us the equation *A^{T}Ax=A^{T}b*. The right hand side *A^{T}b* is a vector, and in-fact the whole equation is in the form *Ax=b* which we know how to solve (again, solving for =x= here). Also note that *A^{T}A* is always square and singlular - and that b/c *A* was skinny it's actually smaller than *A*.

* The LU case
** Direct method
We take the product *A^{T}A* and the product *A^{T}b* and solve *A^{T}Ax=A^{T}b* just like we solve an *Ax=b* system
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/leastsquares.clj
    (defn lu-direct
      "Solve ATA=ATb directly"
      [linear-system output-vector]
      (let [AT (transpose linear-system)
	    ATA (mmul AT linear-system)
	    ATb (mmul AT output-vector)]
	(gauss/solve-with-lu ATA ATb)))
#+END_SRC

#+BEGIN_SRC clojure
  ;; missile problem from page 232
  (let [A [[1.0 0.0 0.0]
	   [1.0 0.25 0.0625]
	   [1.0 0.50 0.25]
	   [1.0 0.75 0.5625]
	   [1.0 1.00 1.0]]
	b [0.000 0.008 0.015 0.019 0.02]]
    (lu-direct A b))
  ;; (-2.2857142857142797E-4 0.03982857142857144 -0.019428571428571444)
#+END_SRC
** The numerically stable solution
Unfortunately while the previous solutions is very simple, it does involve a matrix multiplication and therefore has some numerical issues. A nice trick is presented in *Exercise 4.6.9* with the equations

\begin{equation}
\begin{bmatrix}
I_{m*m} & A\\
A^T & 0_{n*n}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\end{bmatrix}
=
\begin{bmatrix}
b\\
0\\
\end{bmatrix}
\end{equation}

This special block has the property that it'll always be square no matter what shape *A* is. If you multiply out the block matrices you will get two equations
\begin{equation}
\begin{bmatrix}
x_1+Ax_{2}\\
A^{T}x_{1}\\
\end{bmatrix}
=
\begin{bmatrix}
b\\
0\\
\end{bmatrix}
\end{equation}

\begin{equation}
x_1+Ax_{2} = b
\end{equation}

\begin{equation}
A^{T}x_{1} = 0
\end{equation}

Note how the second equation tell us *x_{1}* is in the *N(A^{T})*

Now we multiple both sides of our first equation by *A^{T}*

\begin{equation}
A^{T}(x_1+Ax_{2}) = A^{T}b
\end{equation}

\begin{equation}
A^{T}x_1+A^{T}Ax_{2} = A^{T}b
\end{equation}

\begin{equation}
A^{T}Ax_{2} = A^{T}b
\end{equation}

We just used the fact that *x_{1}* was in the nullspace of *A^{T}* to drop the first term and now we see how *x_{2}* is also a solution to the /least squares/ equation

So we can use any method that works on solving square matrices, use it on our jumbo matrix and get a solution for the least squares problem. All without computing *A^{T}A*.

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/leastsquares.clj
  (defn- jumbo-matrix
    "Use the big block matrix method to compute the least squares solution"
    [input-matrix]
    (let [num-rows (row-count input-matrix)
	  num-columns (column-count input-matrix)
	  identity (identity-matrix num-rows)
	  AT (transpose input-matrix)
	  zeroes (zero-matrix num-columns
			      num-columns)]
      (join-along 0
		  (join-along 1
			      identity
			      input-matrix)
		  (join-along 1
			      AT
			      zeroes))))

  (defn- pad-with-zeroes
    "Takes the INPUT-VECTOR and make it longer with some zeroes padded on the end"
    [input-vector number-of-zeroes]
    (join-along 0
		input-vector
		(zero-vector number-of-zeroes)))

  (defn lu-jumbo
    ""
    [input-matrix output-vector]

    (subvector (gauss/solve-with-lu (jumbo-matrix input-matrix)
				    (pad-with-zeroes output-vector
						     (column-count input-matrix)))
	       (row-count input-matrix)
	       (column-count input-matrix)))
#+END_SRC

#+BEGIN_SRC clojure
  ;; missile problem from page 232 but redone with the big matrix method
  (let [A [[1.0 0.0 0.0]
	   [1.0 0.25 0.0625]
	   [1.0 0.50 0.25]
	   [1.0 0.75 0.5625]
	   [1.0 1.00 1.0]]
	b [0.000 0.008 0.015 0.019 0.02]]
    (lu-jumbo A b))
  ;; [-2.2857142857142797E-4 0.03982857142857144 -0.019428571428571444]
#+END_SRC

