#+TITLE: Linear Systems - Part 2:  Clojure
#+DESCRIPTION: Some linear algebra in Clojure

#+EXPORT_FILE_NAME: index.html
#+HTML_DOCTYPE: html5
#+HTML_LINK_UP: ..
#+HTML_LINK_HOME: ..
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../web/worg.css" />
#+HTML_HEAD_EXTRA: <link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
#+HTML_MATHJAX: path: "../MathJax/MathJax.js?config=TeX-AMS_CHTML"
#+OPTIONS: html-style:nil
#+OPTIONS: num:nil

* Preface
This is a continuation (with some overlap) of what I have been developing in [[http://geokon-gh.github.io/linearsystems-part1/index.html][Part 1]]. There I had developed a linear algebra system from scratch in ELisp and showed how to use it in several different fundamental applications. However as the algorithms become more complicated, the code started to get a little out of hand. A larger fraction of the time and code was spent on helper functions and extending the system to support different operations I needed and certain design simplifications and errors I had made at the beginning made the ultimate system inflexible and a bit unsatifying to work with.

To carry on working a bit less encumbered with my own mistakes, I've switched over to Clojure and it's default ~core.matrix~ library. This already handles many little details I was lacking in my ELips implementation and as you'll see writing new algorithms will be much smoother. There will still need to be helper functions written, and there are probably places where I misuse the library due to ignorance. If you see any mistakes or room for improvement, please file an issue in the repo

As before, this is an org document and can be tangled into the full Clojure project without the need for any external files. The tangled output will also be tracker in the repository, (but is no necessary is you use Emacs)

Explaining Clojure is outside the scope of this project, but in short you will need to install Java and [[http://leiningen.org/][Leiningen]]. After you have both, you just clone this repository, go into it, and run ~lein run~ to run the project or ~lein repl~ to start an interactive REPL session.

* Project managment
Project management in Clojure is done through a top level ~project.clj~ file which specified project details and the dependencies we will need. In our case it's just ~core.matrix~
#+BEGIN_SRC clojure :results output silent :session :tangle project.clj
(defproject linearsystems-part2 "0.1.0-SNAPSHOT"
  :description "linea-systems in Clojure"
  :url "http://geokon-gh.github.io/linearsystems-part2/index.html"
  :license {:name "Eclipse Public License"
            :url "http://www.eclipse.org/legal/epl-v10.html"}
  :dependencies [
                 [org.clojure/clojure "1.10.0"]
                 [net.mikera/core.matrix "0.62.0"]]
  :main ^:skip-aot linearsystems-part2.core
  :target-path "target/%s"
  :profiles {:uberjar {:aot :all}})

#+END_SRC
The rest of the code will live in ~src/core.clj~ as is the convention (maybe if there is a lot of code or parts to break off, these will be in separate namespaces/files..)
We start by declaring the project namespace and including all of ~core.matrix~. Since we will be using it extensively and I don't want to overload any of its names, I'm not even bothering to alias it.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (ns linearsystems-part2.core
    (:require [clojure.core.matrix :refer :all])  ;[denisovan.core :as den]
    (:gen-class))

  (defn -main
    "I don't do a whole lot ... yet."
    [& args]
    (println "Hello, World!"))

#+END_SRC
The ~-main~ is just a placeholder for the moment
* Householder QR
An alternate method to the Gram-Schmidt is to take a more direct approach to building a *QR* matrix - similar to how we worked on building the *LU*. We will just directly zero out columns to build an upper triangular *R*. The difference from the *LU* is that now instead of using elementary matrices to do row operations to get zeroes we will restrict ourselves to using *elementary reflectors*. Their key property is that they are orthonormal, so when we carry out a series of reflections *Q_{1}Q_{2}..Q_{k}A* we can combine them into one matrix which will be guaranteed to be orthonormal as well. Conceptually this is saying that if you do a bunch of reflections one after another, it all adds up in the end to be one reflection - and this makes some intuitive sense. In the *LU*'s Gaussian elimination the elementary matrices we used were neither orthogonal nor normal and we didn't have this same guarantee that the product of several elementary matrices gives another elementary matrix.


** elementary reflector

An /elementary reflector/ does what's written on the label, it's a matrix/linear-system which when given a vector produces its reflection across a /N-1/ dimensional hyperplane. In the next section we will deal with selecting the correct hyperplane, but for the time being we will just focus on building such a matrix. 

The first task is finding a nice concise mechanism to define a hyperplane. If our space is *R^N* then the hyperplane will be *N-1* dimensions and at first blush we seem to need *N-1* vectors to define it. For instance in *3D* space any two vectors not on the same line will define a *2D*. Where *plane = a*v_1 + b*v_2* /for all a and b/. But this method doesn't really scale b/c as *N* increases so does the number of vectors you need. The shortcut is that actually all planes have vectors orthogonal to the hyper plane. These vectors all lie on the same line and we just choose one, call it *u*, and let it represent that remaining *N^{th}* dimension. Now you can simply say that the hyperplane is all vectors orthogonal to *u*. Or more formally, all vectors /not-in-the-span/ of *u* are the hyperplane

Now that we have a way to define a plane we need to work through the mechanics of relfecting an arbitrary vector *x* across the hyperplane. The key insight here is that *x* can be treated as two separate vectors. One that lies in the plane and one that is orthogonal to the plane. The component that lies in the plane is unaffected by the reflection while the component that is orthogonal is basically flipped to point to the other side of the plane. To do this procedure mathematically we take the component of *x* in the direction of the *u* that defines our plane and then we subtract it twice from *x*. This will give us a new vector that points at its own reflection on the other side of the plane. Breaking it down further, the steps go as followed:
 -  *u^{t}x*/||u||* is the amount of *x* in the direction of *u* (a scalar)
 -  *uu^{t}x/||u||^{2}* is the component *x* in the direction of *u* (a vector)
 -  Here we notice that we can subsitute the inner product *u^{t}u* for *||u||^2*
 -  *uu^{t}x/u^{t}u*
 -  *x - 2uu^{t}x/u^{t}u* is you subtracting that vector component twice to get its reflection
 -  *(I-2uu^{t}/u^{t}u)x* is how we'd factor the *x* back out
 -  *(I-2uu^{t}/u^{t}u)* represents the reflector matrix. It's defined uniquely by the *u* vector!
Note that now that we factored out this matrix that given any new value of *x* we can multiply it by *(I-2uu^{t}/u^{t}u)* and get its reflection across the hyperplane orthogonal to *u*. 

*Note:* We will see in the next section that we can't safely assume *u* is unit length - otherwise we could drop the normalizing inner product term and everything would look much cleaner.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn elementary-reflector
    "Build a matrix that will reflect vector across the hyperplane orthogonal to REFLECTION-AXIS"
    [reflection-axis]
    (let [dimension (dimension-count reflection-axis 0)]
      (sub (identity-matrix dimension)
           (mul (outer-product reflection-axis reflection-axis)
                (/ 2 (length-squared reflection-axis))))))
#+END_SRC

** elementary coordinate reflector
Circling back to our original intent, we were trying to use reflectors to clear rows and build an upper triangular matrix (the *Q* in the *QR*). 

The first thing we want to do is have a method to zero out the first column of a matrix, ie *A_{ - ,1}*, If we could build a special elementary reflector *Q_{1}* that reflected that first column on to the elementary vector *e_{1}* (that's *[ 1 0 0 0.. 0 ]* ), then *Q_{1}A* would leave everything under the the first column zeroed out

Generalizing a bit further, we will write a function that take a given vector and a target coordinate axis and produce a reflection matrix that takes one to the other. This is a bit of an inversion of what we did in the previous section. Instead of taking a hyperplane and reflecting over it, we now know what we want to reflect and where we want to reflect it to - we just need to find the right plane to do it. The answer isn't all that complicated, but it's a bit hard to picture. We want to find the plane that lies between where we start and where we want to reflect to. If you have two vectors and you want to make a plane that goes right between them, then all you need to do is make the two vectors the same length and add them together. This will give you a vector that goes right between the two. If you picture it in 2D space then the two vectors add up into a diamond shape with the point lieing on the bisecting line. 

The last catch is that to define out plane we actually want the orthogonal vector. However, if instead you subtract these two equal-length vectors, then you will find that you get a vector orthogonal to that bisection vector/plane.

\begin{equation}
u = x - ||x||e_{1}
\end{equation}

Once you have the orthogonal vector to the bisecting plane, you just feed it into our previous function and get the reflection matrix!

#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn elementary-coordinate-reflector
   "Build a matrix that will reflect the INPUT-VECTOR on to the COORDINATE-AXIS"
   [input-vector coordinate-axis] 
   (let [vector-orthogonal-to-reflection-plane
         (sub input-vector
              (mul coordinate-axis
                   (length input-vector)))]
     (if (zero-matrix? vector-orthogonal-to-reflection-plane)
       ;; degenerate case where the input is on the coordinate axis
       (identity-matrix (dimension-count input-vector 0))
       ;; normal case
       (elementary-reflector vector-orthogonal-to-reflection-plane))))

#+END_SRC
** Zeroing the first column

Now putting all the pieces together, given some matrix *A* we can get back a reflector to zero out its first column


#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn first-column-reflector
    "Build a matrix that will reflect the INPUT-VECTOR on to the first elementary vector [ 1 0 0 .. 0 ]"
    [input-vector]
    (elementary-coordinate-reflector input-vector
                                            (get-row (identity-matrix (dimension-count input-vector 0)) 0)))
#+END_SRC
Now we hit a bit of a snag. You can use the same method to make some matrix *Q_2* that will zero out the second column, but when you combine the two and try *Q_2Q_1A* you will see that *Q_2* is messing up the first column - so we lose the progress we'd made in the first step. We may have gotten the first column to lie on the coordinate vector after *Q_1A*, but when you reflect it again it moves away from the coordinate vector.

In the *LU* Gaussian Elimination method we didn't have this problem b/c clearing subsequent columns was guaranteed to leave you previous columns intact (it would just shuffling zeroes around). Now this guarantee is gone so we need to think of a new trick.

** Zeroing out the second column

As is usually the trick with these things, the solution is thinking in terms of block matrices. When we say we need to clear the second column we actually mean taking the result of our *Q_1A* and clearing everything under the =(2,2)= position. To approach this we construct a new block matrix for *Q_2* that will have the form

 \begin{equation}
 Q_{2}
 =
 \begin{bmatrix}
 1 & 0\\
 0 & S_{ n-1, m-1 }\\
 \end{bmatrix}
 \end{equation}

Notice how when we multiply this matrix times *Q_1A* the first column is left untouched and this =(n-1,m-1)= submatrix *S* will multiple times a submatrix of *Q_1A* which has the =(2,2)= position now in the =(1,1)= position.


 \begin{equation}
 Q_2(Q_1A)
 =
 \begin{bmatrix}
 1 & 0\\
 0 & S\\
 \end{bmatrix}
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & (Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 =
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & S(Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 \end{equation}

In the resulting matrix the only "new" entry we need to worry about is *S(Q_{1}A)_{n-1,m-1}* - everything else is unchanged. In this submatrix we need to clear the first column because it's the second column of our overall matrix - and we are free to choose any appropriate *S* matrix to do it. At this point the problem of choosing *S* mirrors the process we used to clear the first column - the only difference being that the dimension is one smaller

When tackling the third column we just repeat the process but starting with the *S(Q_{1}A)_{n-1,m-1}* matrix - so the method is starting to show some recursion
But the problem is that the subsequent *Q^{-1}_{i}*'s are not as clean as row operations and the column of zeroes will not get preserved between reflections. In other words *Q^{-1}_{1}* will reflect the first column onto *e_{1}*, but then the second reflector *Q^{-1}_{2}* will reflect it away somewhere else and you will lose those zeroes. So we need to be a little more clever here and find a way to write *Q^{-1}_{2}* so that it preserves the column of *Q^{-1}_{1}*

#+BEGIN_QUOTE
*Note*: That *Q^{-1}_{i}* = *Q^{T}_{i}* = *Q_{i}*  b/c *Q_{i}* is a reflector and therefore it's own inverse (reflecting something twice gets your the original back). So *Q^{-1}_{i}* and *Q_{i}* can be used interchangeably.

Furthermore *Q^{-1}* = *(Q_1 Q_2 Q_3 ... Q_n)^{-1}* = *Q^{-1}_{n} ... Q^{-1}_{3} Q^{-1}_{2} Q^{-1}_{1}* \\
But this /does not/ equal *Q* - so you /can not/ use them interchageably

The notation that follows is admittedly a bit less consistent than I'd like - but the algorithm thought be clear nonetheless (*TODO* clean up..)
#+END_QUOTE
/p. 341/ we can write *Q^{-1}_{2}* ( or just *Q_{2}*) using block matrices (Note that the book chooses to confusingly use the letter *R_{i}* where I'm using *Q_{i}*)

\begin{equation}
Q_{2}
=
\begin{bmatrix}
1 & 0\\
0 & S_{ n-1, m-1 }\\
\end{bmatrix}
\end{equation}

When you look at  *Q_{2}(Q_{1}A)* in block matrix form you see that the first column and row of *(Q_{1}A)* is untouched and this new block *S* is multiplied with a /submatrix/ of *Q_{1}A* (which is the *(Q_{1}A)* matrix with the first row/column removed). We choose this *S* to be another reflection matrix which will zero out the first column of that submatrix - which will be in the /second/ column of *Q_{1}A*.

So a pattern start to emerge. You take a matrix *A* then you zero out the first column, then you take a submatrix, zero out its first column and then get the next smaller submatrix, zero out its first column.. etc. What's left to figure out is how to combine everything back together to get the full *Q^{-1}R* matrices we want.

On the next page (342) the book generalizes this trick to any dimension and shows you how to build any given *Q_{i}* matrix but *do not use this!!*. You could build each *Q_{i}* but there is actually a much better way to build *Q^{-1}*

Imagine we were give the full *QR* for the sub-matrix  - lets call it *Q{s}R_{s}*. In other words the smaller matrix *Q_{s}*  could triangularize the sub-matrix of *Q_{1}A*  entirely in one go.  Well with the help of the previous formula we could put it in the place of *S* and build a matrix that represented *Q_{rest}=Q_{k}..Q_{2}*. Then we just multiply with *Q_{1}* to get the full *Q^{-1}* for *A*


\begin{equation}
Q^{-1} = Q_{k} ... Q_{2} Q_{1}
\end{equation}
\begin{equation}
Q^{-1} = Q_{rest} Q_{1}
\end{equation}

\begin{equation}
\begin{bmatrix}
Q_{rest}\\
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & Q_{s}\\
\end{bmatrix}
\end{equation}

\begin{equation}
\begin{bmatrix}
Q^{-1}\\
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & Q_{s}\\
\end{bmatrix}
\begin{bmatrix}
Q_{1}\\
\end{bmatrix}
\end{equation}


\begin{equation}
\begin{bmatrix}
1 & 0\\
0 & Q_{s}\\
\end{bmatrix}
\begin{bmatrix}
Q_{1}\\
\end{bmatrix}
\begin{bmatrix}
A\\
\end{bmatrix}
=
\begin{bmatrix}
R\\
\end{bmatrix}
\end{equation}

So we just need a simple function to take a *Q_{s}* and pad it with these zeroes to build our *Q_{rest}*
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn raise-rank
    "Add a row and column of zeroes to the top left of a matrix. With a 1 in the top left position (0,0)
    Optionally pass in a RANK variable to pad with that many rows (default: 1)"
    ([input-matrix]
     (raise-rank input-matrix 1))
    ([input-matrix rank]
     (if (zero? rank)
       input-matrix
       (raise-rank
        (join-along 1 (column-matrix (get-column (identity-matrix (inc (row-count input-matrix))) 0))
                    (join-along 0 (row-matrix (zero-vector (column-count input-matrix)))
                                input-matrix))
        (dec rank)))))
#+END_SRC

*R_{s}*, the product of reducing the submatrix *Q_{1}A* can be similarly used to build *R*, however if you break up the *Q_{1}A* into block matrices you will see that the first row of *Q_{1}A* is in effect preserved and needs to be copied over

\begin{equation}
\begin{bmatrix}
1 & 0\\
0 & Q_{s}\\
\end{bmatrix}
\begin{bmatrix}
(Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
(Q_{1}A)_{*,1} & (Q_{1}A)_{s,s}\\
\end{bmatrix}
=
\begin{bmatrix}
(Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
0 & Q_{s}(Q_{1}A)_{s,s}\\
\end{bmatrix}
=
\begin{bmatrix}
(Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
0 & R_{s}\\
\end{bmatrix}
=
\begin{bmatrix}
R\\
\end{bmatrix}
\end{equation}

So we similarly need a little helper function here to "augment" *R_{s}* to *R* but with the first row inserted manually from *Q_{1}A* (done in-code late)
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj

  (defn raise-rank-and-insert-row-column
    "Takes a submatrix and put it's in the lower right corner of a larger matrix.
    The submatrix is 1 row and column smaller.
    First insert a column (size of input-matrix  and then a row (size + 1)"
    [input-matrix insert-column insert-row]
    (join-along 0 (row-matrix insert-row)
                (join-along 1 (column-matrix insert-column)
                            input-matrix)))

  (defn raise-rank-and-insert-row
    "Takes a submatrix and put it's in the lower right corner of a larger matrix.
    The submatrix is 1 row and column smaller
    First insert a column of zeroes and then the passed in row (size + 1)"
        [input-matrix insert-row]
        (raise-rank-and-insert-row-column
         input-matrix
         (zero-vector (column-count input-matrix))
         insert-row))
#+END_SRC

But ofcourse we don't have the *Q_{s}R_{s}* yet, so we need to think of this method recursively. *Q_{s}R_{s}* is just the *Q^{-1}R* of a smaller matrix which we can immediately calculate b/c it's simply the submatrix of *Q_{1}A* and  we have both *Q_{1}* and *A* . Once we have the submatrix, we call this procedure again and again we we will make a new *Q_{1}* - but now for this smaller matrix. Then again we get a *Q_{1}A* for this smaller matrix and keep going over and over - at each step the matrix gets one row and column smaller and at some point you will be left with a single column/row in which case the *Q_{1}* will be the full *Q^{-1}* of *A* and *Q_{1}A = Q^{-1}A = R*. So going up a step you will finally have a  *Q_{s}* and so we know how to build a *Q^{-1}R*. This gives us the *Q_{s}* for the step before that, and we just continue going back and building up our *Q^{-1}R* one submatrix at a time till we are left with the full *Q^{-1}R*

*R* is built up similarly in parallel
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn householder-QR
    "Use reflection matrices to build the QR matrix. Returns a [Q^T R] pair"
    [input-matrix]
    (let [reflector-to-zero-out-first-column
          (first-column-reflector (get-column input-matrix 0))
          input-matrix-with-first-column-zeroed-out
          (mmul reflector-to-zero-out-first-column input-matrix)]
      (if
          ;; Base Case: We're out of columns/rows to reduce
          ;;            Return the reflector and the reduced column
          (or (= (column-count input-matrix) 1) (= (row-count input-matrix) 1))
          [reflector-to-zero-out-first-column input-matrix-with-first-column-zeroed-out]
          ;; Recursive step: Get the Q^{-1}R of the submatrix
          ;;                 Then and combine it with your reflector and reduced matrix
          (let [submatrix (submatrix
                           input-matrix-with-first-column-zeroed-out
                           1 (dec (row-count input-matrix))
                           1 (dec (column-count input-matrix)))
                [submatrix-Q submatrix-R] (householder-QR submatrix)]
            [(mmul (raise-rank submatrix-Q)
                   reflector-to-zero-out-first-column)
             (raise-rank-and-insert-row submatrix-R
                                        (get-row input-matrix-with-first-column-zeroed-out 0))]))))
#+END_SRC

* Least Squares again
While the new *QR* matrices seem to have some very desirable qualities as compared to the *LU*, one major issue is still outstanding. When we perform Gaussian Elimination the upper and lower triangular matrices directly inform us about how to solve the *Ax=b* system of linear equations. Given an output *b* we can use back/forward substitution to pop out an *x* input that satisfies the system of equations. However with the *QR* the *Q* doesn't really make this same method possible b/c it's not triangular.

This is where we need to remember the Least Squared method we'd used previously. In short when a precise solution doesn't exist we try to minimize the difference between *Ax* and *b* by taking the derivative of *(Ax-b)^2*, setting it equal to zero and solving the new system. We found that in matrix notation this gave us *A^{T}Ax=A^{T}b*. We also say (and it should be intuitively apparent) that this gives the exact solution for *Ax=b* when it exist. Now sticking *QR* in for *A* we get *(QR)^{T}QRx=(QR)^{T}b* -> *R^{T}Q^{T}QRx=R^{T}Q^{T}b* and this is where the orthonormality starts to finally pay off! *Q^{T}=Q^{-1}* so *Q{T}Q = I* and so our equations just becomes *R^{T}Rx=R^{T}Q^{T}b* where the right side will evaluate to some some unit column and the left side will be solvable my back/forward substitution again (b/c *R* and *R^{T}* are triangular)

Notice that we did that all in theoretical equation form and how we've avoided having to actually compute *A^{T}A* completely which is a big advantage considering getting the *QR* is more computationally challenging than doing Gaussian Elimination. Pages 346-350 also enumerate the advantages when it comes to numerical stability and computational complexity. However, the augmented matrix trick from *Exercise 4.6.9* is not mentioned.

* Reduction to Hessenberg Form
The *QR* decomposition has given us a great tool for expressing a linear system in a convenient orthogonal basis. The *Q* is the convenient (unique) orthonormal basis and *R* are the coordinates of *A* in this *Q* basis. However if we rewrite *Ax=b* in terms of the *QR* as *QRx=b* we see that *Rx* is not particularly meaningful on it's own b/c it's multiplying coordinates in one basis with a vector in the standard basis.

Looking back at pages ~254~ - ~255~, it seems we should be able to take our input vector *x*, change it to a convenient basis, put it through our linear system, and then go back to the standard basis we started with. The trick will be to just build this basis so that *A* is in an easier/more-convenient form. 

The text start on page ~350~ suggests getting the linear system into the =Upper-Hessenberg Form=, which is /almost upper triangular/  with just one nonzero subdiagonal. The text states that this is much easier than finding an basis that is fully upper-triangfular - and we will see how the Hessenberg for allows us to have a very convenient recursive block matrix solution. The procedure is very similar to how we did the Householder QR decomposition, but with a small surface level change. Whereas before we reduced the first column with a reflector - ie. *Q_{1}A* - now we just need to also "unreflect" the result to get back to the original standard basis. Fortunately this turns out to be very easy b/c the reflectors are their own inverse so we just need to instead write out *Q_{1}AQ_{1}* as *Q_{1}AQ_{1}*.

The complication here is that if we write a Householder reflection here for *Q_{1}* then the diagonal terms in *A*, ie. *A_{i,i}* will all get multiplied and this is for some reason undesirable (*TODO* Understand why this is a drawback..). If we limit to eliminating the sub-sub-diagonal terms then we can write it in block form and avoid this whole issue

So if
\begin{equation}
Q_{1} =
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
\end{equation}

Then we can write out *Q_{1}AQ_{1}* as:

\begin{equation}
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
\begin{bmatrix}
A_{1,1} & A_{1,*}\\
A_{*,1} & A_{sub}\\
\end{bmatrix}
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
=
\begin{bmatrix}
A_{1,1} & A_{1,*} Q_{1,sub}\\
Q_{1,sub}A_{1,*} & Q_{1,sub} A_{1,*} Q_{1,sub}
\end{bmatrix}
=> =>
\begin{bmatrix}
A_{1,1} & A_{1,*} Q_{1,sub}\\
\begin{bmatrix}
1 \\ 0 \\ .. \\ 0
\end{bmatrix}
 & Q_{1,sub} A_{1,*} Q_{1,sub}
\end{bmatrix}
\end{equation}

As before we are looking to reflect the first column onto a coordinate axis so that we get zeroes. Just here we're leaving the diagonal untouched and reflecting the terms under it. So we want the *Q_{1,sub}A_{1,-}* column block matrix product to turn into *[ 1 0 0 0 .. 0 ]* and we can reuse ~first-column-reflector~  to get the appropriate *Q_{1,sub}*. Finally reusing ~raise rank~ we can build *Q_{1}*
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn hessenberg-form-first-partial-reflector
    "Builds a matrix that will reduce the first column of INPUT-MATRIX to  Hessenberg Form"
    [input-matrix]
    (if
        ;; Degenerate Case: 1 x 1 matrix
        (or (= (column-count input-matrix) 1) (= (row-count input-matrix) 1))
      [[ 1 ]]
    (let [first-column (get-column input-matrix 0)
          subdiagonal-column (subvector first-column 1 (dec (row-count first-column)))
          orthogonal-reducer (first-column-reflector subdiagonal-column)]
      (raise-rank orthogonal-reducer))))
#+END_SRC
 Once we've chose our *Q_{1}* we calculate the submatrix *Q_{1} A_{1,*} Q_{1}* and then call the recursively just like last time. Working back up the call stack the matrices are combined pretty much as before. We simply raise the rank of the *Q* matrices and pad the resulting matrices like we did for the resulting *R* matrices before.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn hessenberg-form-reduction
    "Reduce the INPUT-MATRIX to  Hessenberg Form  - H , using reflectors - P. Result will be in the form [P^T H]"
  [input-matrix]
  (let [reflector-to-zero-out-first-column
        (hessenberg-form-first-partial-reflector input-matrix)
        input-matrix-with-first-column-zeroed-out
        (mmul reflector-to-zero-out-first-column input-matrix (transpose reflector-to-zero-out-first-column))]
    (if
        ;; Base Case: We're out of columns/rows to reduce
        ;;            Return the reflector and the reduced column
        (or (= (column-count input-matrix) 1) (= (row-count input-matrix) 1))
        [reflector-to-zero-out-first-column input-matrix-with-first-column-zeroed-out]
        ;; Recursive step: Get the Q^{-1}R of the submatrix
        ;;                 Then and combine it with your reflector and reduced matrix
        (let [submatrix (submatrix
                         input-matrix-with-first-column-zeroed-out
                         1 (dec (row-count input-matrix))
                         1 (dec (column-count input-matrix)))
              [submatrix-P submatrix-H] ( hessenberg-form-reduction submatrix)]
          [(mmul (raise-rank submatrix-P)
                 reflector-to-zero-out-first-column)
           (raise-rank-and-insert-row-column submatrix-H
                                     (subvector (get-column input-matrix-with-first-column-zeroed-out 0) 1 (dec (row-count input-matrix-with-first-column-zeroed-out)))
                                     (get-row input-matrix-with-first-column-zeroed-out 0))]))))
#+END_SRC
* TODOs
- add some TODOs
* SRC_Block template
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn matrix-template
"template"
[matrix]
)
#+END_SRC

* End
#+BEGIN_Q^{-1}UOTE
This webpage is generated from an org-document (at ~./index.org~) that also generates all the files described. 

Once opened in Emacs:\\
- ~C-c C-e h h~ generates the webpage  \\
- ~C-c C-v C-t~ exports the code blocks into the appropriate files\\
- ~C-c C-c~     org-babel-execute-src-block
- ~C-c C-v C-b~ org-babel-execute-buffer
#+END_Q^{-1}UOTE
