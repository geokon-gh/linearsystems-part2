#+TITLE: MoreLinear: some linear algebra in Clojure
#+DESCRIPTION: Some linear algebra in Clojure

#+EXPORT_FILE_NAME: index.html
#+HTML_DOCTYPE: html5
#+HTML_LINK_UP: ..
#+HTML_LINK_HOME: ..
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../web/worg.css" />
#+HTML_HEAD_EXTRA: <link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
#+HTML_MATHJAX: path: "../MathJax/MathJax.js?config=TeX-AMS_CHTML"
#+OPTIONS: html-style:nil
#+OPTIONS: num:nil
#+OPTIONS: html-postamble:nil
#+OPTIONS: html-scripts:nil

* Preface
This is a continuation (with some overlap) of what I have been developing in [[http://geokon-gh.github.io/elinear/index.html][elinear]]. There I had developed a linear algebra system from scratch in ELisp and showed how to use it in several different fundamental applications. However as the algorithms become more complicated, the code started to get a little out of hand. A larger fraction of the time and code was spent on helper functions and extending the system to support different operations I needed and certain design simplifications and errors I had made at the beginning made the ultimate system inflexible and a bit unsatifying to work with.

To carry on working a bit less encumbered with my own mistakes, I've switched over to Clojure and the ~core.matrix~ library. (Note the *core* is meaningless and it's not part of the Clojure standard library) This already handles many little details I was lacking in my ELips implementation and as you'll see writing new algorithms will be much smoother. There will still need to be helper functions written, and there are probably places where I misuse the library due to ignorance. If you see any mistakes or room for improvement, please file an issue in the repo

Note that ~core.matrix~ is more of a "front end" API and it provides many different backends, some on the JVM others in JS. It's also not extremely performance focused and you can very easily end up doing operations that are very slow.

If you are concerned about getting as much as you can out of your system then I suggest looking at the ~neanderthal~ library which provides a thin wrapper around BLAS. It makes it very easy to work with BLAS and if you're on an x64 system this is the best CPU based performance you can hope for really. This project has a ~neanderthal~ branch where I starter implementing a few of the first functions. It should give you a good taste of what working in the more constrained BLAS environment looks like. It's an interesting work flow and really forces you to think a lot harder about your algorithms

However my goal here is mostly educational and I'm interested in running on as many platforms as possible. As before, this is an org document and can be tangled into the full Clojure project without the need for any external files. The tangled output is tracker in the repository so you can clone the project and launch a REPL without touching Emacs.

Explaining Clojure is outside the scope of this project!

* Project managment
*TODO* Migrate to tools.deps
Project management in Clojure is done through a top level ~project.clj~ file which specified project details and the dependencies we will need. In our case it's just ~core.matrix~
#+BEGIN_SRC clojure :results output silent :session :tangle project.clj
(defproject morelinear "0.1.0-SNAPSHOT"
  :description "linea-systems in Clojure"
  :url "http://geokon-gh.github.io/morelinear/index.html"
  :license {:name "Eclipse Public License"
            :url "http://www.eclipse.org/legal/epl-v10.html"}
  :dependencies [
                 [org.clojure/clojure "1.10.0"]
                 [net.mikera/core.matrix "0.62.0"]]
  :main ^:skip-aot morelinear.core
  :target-path "target/%s"
  :profiles {:uberjar {:aot :all}})

#+END_SRC
The rest of the code will live in ~src/core.clj~ as is the convention (maybe if there is a lot of code or parts to break off, these will be in separate namespaces/files..)
We start by declaring the project namespace and including all of ~core.matrix~. Since we will be using it extensively and I don't want to overload any of its names, I'm not even bothering to alias it.
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (ns morelinear.core
    (:require [clojure.core.matrix :refer :all])  ;[denisovan.core :as den]
    (:gen-class))

  (defn -main
    "I don't do a whole lot ... yet."
    [& args]
    (println "Hello, World!"))

#+END_SRC
The ~-main~ is just a placeholder for the moment
* Householder QR
The Householder decomposition is An alternate method to the Gram-Schmidt decomposition for build a *QR* matrix pair. instead of building an orthonormal basis it takes a more direct approach similar to the Gaussian *LU* procedure by just zeroing out columns to get the desired shape. The difference from the *LU* is that now instead of using elementary matrices to do row operations to get zeroes we will restrict ourselves to using *elementary reflectors*.

** elementary reflector

An /elementary reflector/ does what's written on the label, it's a matrix/linear-system which when given a vector produces its reflection across a /N-1/ dimensional hyperplane (in 2D it's a line and in 3D it's a plane). In the next section we will deal with selecting the correct hyperplane, but for the time being we will just focus on building such a matrix. 

Their key property is that they're both /orthonormal/ and /symmetric/.
- *Orthonormal* :: their rows (and columns) form an orthogonal basis and each row (and column) is of unit length. As a consequence, given an orthonormal matrix *Q* its inverse will be its transpose. In *QQ^{T}* the off-diagonal terms will be inner products of different basis vectors. Since they're orthogonal these inner product will be zero. The diagonal elements will be inner products of vectors with themselves so it will equal to their length. Since the rows are all unit length the diagonal will be all ones. 
- *Symmetric* :: means that it's equal to its own transpose. As a consequence a /symmetric/ + /orthonormal/ matrix will be it's own inverse. ie. *QQ=I*. This makes some sense b/c reflecting something twice gives you the same things back.
- *A product of orthonormal matrices will be orthonormal* :: b/c given two orthonormal matrices *U* and *V* we can test for orthonormality: *(UV)(UV)^{T}=UVV^{T}U^{T}=I*

So if we carry out a series of reflections *Q_{k}..Q_{2}..Q_{1}A* we can combine them into one matrix *Q* which will be guaranteed to be orthonormal as well. It however /will not necessarily/ be a reflection matrix!


To build a reflector matrix we need to find a nice concise mechanism to define the hyperplane over which it reflects. If our space is *R^N* then the hyperplane will be *N-1* dimensions and at first blush we seem to need *N-1* vectors to define it. For instance in *3D* space any two vectors not on the same line will define a 2D plane (ie. *a*v_1 + b*v_2* for all /a/ and /b/). But this method doesn't really scale b/c as *N* increases so does the number of vectors you need. The shortcut is that actually all planes have vectors orthogonal to the hyper plane. These vectors all lie on the same line and we can just choose one, call it *u*, and let it represent that remaining *N^{th}* dimension. Now you can simply say that the hyperplane is all the vectors orthogonal to *u*. Or more formally, all vectors /not-in-the-span/ of *u* are the hyperplane

Now that we have a way to define a plane we need to work through the mechanics of relfecting an arbitrary vector *x* across the hyperplane. *x* can be broken up into two separate vectors: One that lies in the plane and one that is orthogonal to the plane. The component that lies in the plane is unaffected by the reflection while the component that is orthogonal is in the direction of *u* and is flipped by just getting its negative. To do this procedure mathematically we start with a vector *x* and subtract twice its component in the direction of the *u*:
 -  *u^{t}x*/||u||* is the amount of *x* in the direction of *u* (a scalar)
 -  *uu^{t}x/||u||^{2}* is the component *x* in the direction of *u* (a vector)
 -  Here we notice that we can subsitute the inner product *u^{t}u* for *||u||^2*
 -  *uu^{t}x/u^{t}u*
 -  *x - 2uu^{t}x/u^{t}u* is you subtracting that vector component twice to get its reflection
 -  *(I-2uu^{t}/u^{t}u)x* is how we'd factor the *x* back out
Notice how in the last step we managed to factor out the *x* , so we can subsitute it with any other vector to get a reflection. The matrix *(I-2uu^{t}/u^{t}u)* to its left is the reflector matrix. It's defined uniquely by *u* and is independent of *x*.

*Note:* We will see in the next section that thought it would make life easier, we can't safely assume *u* is unit length
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn elementary-reflector
    "Build a matrix that will reflect vector across the hyperplane orthogonal to REFLECTION-AXIS"
    [reflection-axis]
    (let [dimension (dimension-count reflection-axis 0)]
      (sub (identity-matrix dimension)
	   (mul (outer-product reflection-axis reflection-axis)
		(/ 2 (length-squared reflection-axis))))))
#+END_SRC
For example:
#+BEGIN_SRC clojure
  (pm (elementary-reflector [43.0 36.0 38.0 90.0]))
  ;; [[ 0.709 -0.244 -0.258 -0.610]
  ;;  [-0.244  0.796 -0.216 -0.511]
  ;;  [-0.258 -0.216  0.772 -0.539]
  ;;  [-0.610 -0.511 -0.539 -0.277]]
  ;; nil


  (pm (mmul (elementary-reflector [43.0 36.0 38.0 90.0])
	    [43.0 36.0 38.0 90.0]))
  ;; [-43.000 -36.000 -38.000 -90.000]
  ;; nil
#+END_SRC

** elementary coordinate reflector
Circling back to our original intent, we were trying to use reflectors to clear rows and build an upper triangular matrix (the *R* in the *QR*)

The first thing we want to do is have a way to zero out the first column of a matrix, ie *A_{ - ,1}*. We'd like to build a special elementary reflector *Q_{1}* that reflected that first column on to the elementary vector *e_{1}* (that's *[ 1 0 0 0.. 0 ]* ). If we had this matrix then *Q_{1}A* would leave everything under the the first column zeroed out.

Generalizing the problem a bi, this is a bit of an inversion of what we did in the previous section. Instead of taking a hyperplane and reflecting over it, we now know what we want to reflect and where we want to reflect it to - we just need to find the right plane to do it. This plane lies between where we start and where we want to reflect to. If you picture it in 2D space then you could take the two vectors add up their norms and you will get a vector that bisects them (forming a equilateral diamond shape with the point lieing on the bisecting line). In higher dimensions it will get a bit more complicated as you need more and more vectors. 

Fortunately we know we can define the plane with the orthogonal vector. To get that we just subtract the two vector norms and you will find that you get a vector orthogonal to that bisection vector/plane.

\begin{equation}
u = x - ||x||e_{1}
\end{equation}

Strain your brain and try to picture it in 2D and in 3D and it should make sense.

*TODO*: Maybe add a picture..

Once you an orthogonal vector to the bisecting plane, you just feed it into our previous function and get the reflection matrix

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn elementary-coordinate-reflector
   "Build a matrix that will reflect the INPUT-VECTOR on to the COORDINATE-AXIS"
   [input-vector coordinate-axis] 
   (let [vector-orthogonal-to-reflection-plane
         (sub input-vector
              (mul coordinate-axis
                   (length input-vector)))]
     (if (zero-matrix? vector-orthogonal-to-reflection-plane)
       ;; degenerate case where the input is on the coordinate axis
       (identity-matrix (dimension-count input-vector 0))
       ;; normal case
       (elementary-reflector vector-orthogonal-to-reflection-plane))))

#+END_SRC
For instance we can take some random vector and say we want to reflect it onto the *e_1*
#+BEGIN_SRC clojure
  (pm (elementary-coordinate-reflector [24 77 89 12]
				       [1 0 0 0]))
  ;; [[0.199  0.638  0.737  0.099]
  ;;  [0.638  0.492 -0.587 -0.079]
  ;;  [0.737 -0.587  0.321 -0.091]
  ;;  [0.099 -0.079 -0.091  0.988]]
  ;; nil
#+END_SRC
We got some seemingly random matrix out. If we then multiply it times out random vector, it reflects perfectly to *e_1*
#+BEGIN_SRC clojure
  (pm (mmul (elementary-coordinate-reflector [24 77 89 12]
					     [1 0 0 0])
	    [24 77 89 12])))
  ;; [120.706 -0.000 -0.000 -0.000]
  ;; nil
#+END_SRC


** Zeroing the first column

Now putting all the pieces together, given some matrix *A* we can get back a reflector to zero out its first column

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn first-column-reflector
    "Build a matrix that will reflect the INPUT-MATRIX on to the first elementary vector [ 1 0 0 .. 0 ]"
    [input-matrix]
    (elementary-coordinate-reflector (get-column input-matrix
						 0)
				     (get-row (identity-matrix (dimension-count input-matrix 0)) 0)))
#+END_SRC
This is really just a wrapper for the previous function. Now we can test it by writing out a random matrix and zeroing out its first column
#+BEGIN_SRC clojure
  (pm (first-column-reflector [[43.0 36.0 38.0 90.0]
			       [21.0 98.0 55.0 48.0]
			       [72.0 13.0 98.0 12.0]
			       [28.0 38.0 73.0 20.0]]))
  ;; [[0.473  0.231  0.792  0.308]
  ;;  [0.231  0.899 -0.348 -0.135]
  ;;  [0.792 -0.348 -0.192 -0.463]
  ;;  [0.308 -0.135 -0.463  0.820]]
  ;; nil


  (let [A [[43.0 36.0 38.0 90.0]
	   [21.0 98.0 55.0 48.0]
	   [72.0 13.0 98.0 12.0]
	   [28.0 38.0 73.0 20.0]]]
    (pm (mmul (first-column-reflector A)
	      A)))
  ;; [[90.874  61.690 130.830 69.349]
  ;;  [ 0.000  86.731  14.280 57.059]
  ;;  [-0.000 -25.637 -41.613 43.058]
  ;;  [ 0.000  22.975  18.706 32.078]]
  ;; nil
#+END_SRC

** Zeroing out the second column and so on..

Now we hit a bit of a problem. You can use the same method to make some matrix *Q_2* that will zero out the second column, but when you combine the two and try doing *Q_{2}Q_{1}A* you will see that *Q_{2}* is messing up the first column - so we lose the progress we'd made in the first step. We may have gotten the first column to lie on the coordinate vector after *Q_{1}A*, but when you reflect it again it moves away from the coordinate vector b/c all columns are reflected at each step.

In the *LU* Gaussian Elimination method we didn't have this problem b/c clearing subsequent columns was guaranteed to leave you previous columns intact (b/c shuffling rows would just be moving around zeroes from the pervious columns). Now this guarantee is gone so we need to find a way to reflect some matrix columns and not others

The solution is thinking in terms of block matrices. When we say we need to clear the second column we can spell that out as : we want to take the result of our first reflector *Q_{1}A* and now clearing everything under the =(2,2)= position. To avoid touching the first column we construct *Q_{2}* with the following form:

 \begin{equation}
 Q_{2}
 \\=
 \begin{bmatrix}
 1 & 0\\
 0 & S_{ n-1, m-1 }\\
 \end{bmatrix}
 \end{equation}

Notice how when we multiply this matrix times *Q_{1}A* the first column is left untouched


 \begin{equation}
 Q_2(Q_1A)
 \\=
 \begin{bmatrix}
 1 & 0\\
 0 & S\\
 \end{bmatrix}
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & (Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 \\=
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & S(Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 \end{equation}

Now also notice that the =n-1 by m-1= submatrix *S* will multiple times a submatrix of *Q_{1}A* which has that =(2,2)= position now in the =(1,1)= position.

We've also got a bit of bonus b/c in the resulting matrix the only "new" entry we need to worry about is *S(Q_{1}A)_{n-1,m-1}* - the first column and row have remained the same. In this submatrix product we need to again clear the first column because it's the second column of our overall matrix. Choosing an appropriate *S* matrix to do it mirrors the process we used to clear the first column of *A* - the only difference being that the dimension is one smaller.

When tackling the third column we do this again, getting the next submatrix of *S(Q_{1}A)_{n-1,m-1}*. At each step we are reducing the first column, grabbing the result's submatrix and calling the procedure again - until we are out of things to reduce

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn reduce-to-r
    "Reduce a matrix to a lower triangular orthonormal matrix"
    [input-matrix]
    (if (or (= 1 (row-count input-matrix))
	    (= 1 (column-count input-matrix))) 
      input-matrix ;; base case
      (do (assign! input-matrix
		   (mmul (first-column-reflector input-matrix)
			 input-matrix))
	  (recur (submatrix input-matrix
			    1
			    (dec (row-count input-matrix))
			    1
			    (dec (column-count input-matrix)))))))
#+END_SRC

#+BEGIN_QUOTE
*Note*: 
- This ~submatrix~ function is interesting b/c it will not make a copy of the matrix. Instead it will return a matrix object that shares its underlying data/memory with the parent matrix. So as we reduce the submatrices, the orginal matrix is being reduced as well
- The ~mmul~ matrix multiplication will unfortunately produce a temporary intermediary matrix which will then get copied into the matrix/submatrix. Other more advanced matrix libraries may have ways to do this in-place.
#+END_QUOTE

Now to test it I'm reusing the same random matrix from the previous example:
#+BEGIN_SRC clojure
  (def A (mutable [[43.0 36.0 38.0 90.0]
		   [21.0 98.0 55.0 48.0]
		   [72.0 13.0 98.0 12.0]
		   [28.0 38.0 73.0 20.0]]))

  (pm A)
  ;; [[43.000 36.000 38.000 90.000]
  ;;  [21.000 98.000 55.000 48.000]
  ;;  [72.000 13.000 98.000 12.000]
  ;;  [28.000 38.000 73.000 20.000]]
  ;; nil
  (reduce-to-r A)

  (pm A)
  ;; [[90.874 61.690 130.830  69.349]
  ;;  [ 0.000 93.313  29.311  49.102]
  ;;  [-0.000  0.000  37.767 -48.089]
  ;;  [ 0.000 -0.000   0.000 -37.619]]
  ;; nil
#+END_SRC
If you looking at the result I got when running ~(first-column-reflector ..)~  then you'll see that the first column and row have been preserved as we expect.

** Getting the reflectors

The last section managed to get the *R* in the *QR*. The next step is combining all these intermediary relfectors (those *S* matrices) into a matrix *Q*

This step it unfortunately not quite as elegant as the reduction (or I haven't found the right solution!). 

The easiest solution is to work backwards from the last iteration step where there reflector matrix is just *1*. Then going back up one iteration we would take *1* and combine it with the =first-column-reflector= we got at that step.. and so on up the iterations till we got to back to the *Q_{1}*. Unfortunately with this method we build up *Q* as we work back up the stack and after we have finished the reduction. So as we reduce we need to keep around all these intermediary reflector till we get the last one (the *1*). Only then can we combine them.

The better but uglier solution is to combine the reflectors as we go, starting with *Q_{1}*. At each iteration of the reduction we got a new ~(first-column-reflector .. )~ , and just like with *S* in the 2^{nd} column case, we pad the matrix and make it =n by n=

 \begin{equation}
 Q_{k}
 \\=
 \begin{bmatrix}
 I_{k-1,k-1} & 0\\
 0 & S_{n-k+1,n-k+1}\\
 \end{bmatrix}
 \end{equation}

In ~core.matrix~ there is a convenient ~(block-diagonal-matrix .. )~ function to handle making these

So far we've been looking at *Q_{1}Q_{2}* .. *Q_{n}A=R*, but ultimately we want to get to *A=QR*. The reflectors are their own inverse, so written out like that the equation remains easily invertable. While by contrast *Q^{-1}A=R* is not so easy to invert... b/c *Q^{-1}* may not be a reflector at all. So we flip the equation ahead of time *A=Q_{n}* .. *Q_{2}Q_{1}R* and we make sure to build *Q* in the right order *Q=Q_{1}Q_{2}* .. *Q_{1}*

#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn householder-reduce-to-QR
    "Increase the dimension of a reflector by padding it with an identity matrix"
    [reduction-matrix input-matrix]
    (if (or (= 0 (row-count input-matrix))
	    (= 0 (column-count input-matrix)))
      reduction-matrix ;; base case
      (let [reflector (first-column-reflector input-matrix)]
	(do (assign! input-matrix
		     (mmul reflector
			   input-matrix))
	    (recur (mmul reduction-matrix
			 (block-diagonal-matrix [(identity-matrix (- (row-count reduction-matrix)
								     (row-count input-matrix)))
						 reflector]))
		   (submatrix input-matrix
			      1
			      (dec (row-count input-matrix))
			      1
			      (dec (column-count input-matrix))))))))

  (defn householder-QR
    "A wrapper for the real function"
    [input-matrix]
    (householder-reduce-to-QR (identity-matrix (row-count input-matrix))
			      input-matrix))
#+END_SRC
The function reduces the input matrix to *R* and returns *Q*.
#+BEGIN_SRC clojure
  (def A (mutable [[43.0 36.0 38.0 90.0]
		   [21.0 98.0 55.0 48.0]
		   [72.0 13.0 98.0 12.0]
		   [28.0 38.0 73.0 20.0]]))

  (def Q (householder-QR A))
  (pm Q)
  ;; [[0.473  0.073 -0.690 -0.543]
  ;;  [0.231  0.897 -0.041  0.374]
  ;;  [0.792 -0.384  0.149  0.450]
  ;;  [0.308  0.204  0.708 -0.602]]
  (pm A)
  ;; [[90.874 61.690 130.830  69.349]
  ;;  [ 0.000 93.313  29.311  49.102]
  ;;  [-0.000  0.000  37.767 -48.089]
  ;;  [ 0.000 -0.000   0.000 -37.619]]
  (pm (mmul Q A))
  ;; [[43.000 36.000 38.000 90.000]
  ;;  [21.000 98.000 55.000 48.000]
  ;;  [72.000 13.000 98.000 12.000]
  ;;  [28.000 38.000 73.000 20.000]]
#+END_SRC

* Least Squares again
While the new *QR* matrices seem to have some very desirable qualities as compared to the *LU*, one major issue is still outstanding. When we perform Gaussian Elimination the upper and lower triangular matrices directly inform us about how to solve the *Ax=b* system of linear equations. Given an output *b* we can use back/forward substitution to pop out an *x* input that satisfies the system of equations. However with the *QR* the *Q* doesn't really make this same method possible b/c it's not triangular.

This is where we need to remember the Least Squared method we'd used previously. In short when a precise solution doesn't exist we try to minimize the difference between *Ax* and *b* by taking the derivative of *(Ax-b)^2*, setting it equal to zero and solving the new system. We found that in matrix notation this gave us *A^{T}Ax=A^{T}b*. We also say (and it should be intuitively apparent) that this gives the exact solution for *Ax=b* when it exist. Now sticking *QR* in for *A* we get *(QR)^{T}QRx=(QR)^{T}b* -> *R^{T}Q^{T}QRx=R^{T}Q^{T}b* and this is where the orthonormality starts to finally pay off! Since *Q^{T}=Q^{-1}* and *Q^{T}Q = I* our equations just becomes *R^{T}Rx=R^{T}Q^{T}b* where the right side will evaluate to some some unit column and the left side will be solvable my back/forward substitution again (b/c *R* and *R^{T}* are triangular)

Notice that we did that all in theoretical equation form and how we've avoided having to actually compute *A^{T}A* completely which is a big advantage considering getting the *QR* is more computationally challenging than doing Gaussian Elimination. Pages 346-350 also enumerate the advantages when it comes to numerical stability and computational complexity. However, the augmented matrix trick from *Exercise 4.6.9* is not mentioned.

* Reduction to Hessenberg Form
The *QR* decomposition has given us a great tool for expressing a linear system in a convenient orthogonal basis. The *Q* is the convenient (unique) orthonormal basis and *R* are the coordinates of *A* in this *Q* basis. However if we rewrite *Ax=b* in terms of the *QR* as *QRx=b* we see that *Rx* is not particularly meaningful on it's own b/c it's multiplying coordinates in one basis with a vector in the standard basis.

Looking back at pages ~254~ - ~255~, it seems we should be able to take our input vector *x*, change it to a convenient basis, put it through our linear system, and then go back to the standard basis we started with. The trick will be to just build this basis so that *A* is in an easier/more-convenient form. 

The text start on page ~350~ suggests getting the linear system into the =Upper-Hessenberg Form=, which is /almost upper triangular/  with just one nonzero subdiagonal. The text states that this is much easier than finding an basis that is fully upper-triangfular - and we will see how the Hessenberg for allows us to have a very convenient recursive block matrix solution. The procedure is very similar to how we did the Householder QR decomposition, but with a small surface level change. Whereas before we reduced the first column with a reflector - ie. *Q_{1}A* - now we just need to also "unreflect" the result to get back to the original standard basis. Fortunately this turns out to be very easy b/c the reflectors are their own inverse so we just need to instead write out *Q_{1}AQ_{1}* as *Q_{1}AQ_{1}*.

The complication here is that if we write a Householder reflection here for *Q_{1}* then the diagonal terms in *A*, ie. *A_{i,i}* will all get multiplied and this is for some reason undesirable (*TODO* Understand why this is a drawback..). If we limit to eliminating the sub-sub-diagonal terms then we can write it in block form and avoid this whole issue

So if
\begin{equation}
Q_{1} =
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
\end{equation}

Then we can write out *Q_{1}AQ_{1}* as:

\begin{equation}
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
\begin{bmatrix}
A_{1,1} & A_{1,*}\\
A_{*,1} & A_{sub}\\
\end{bmatrix}
\begin{bmatrix}
1 & 0\\
0 & Q_{1,sub}\\
\end{bmatrix}
=
\begin{bmatrix}
A_{1,1} & A_{1,*} Q_{1,sub}\\
Q_{1,sub}A_{1,*} & Q_{1,sub} A_{1,*} Q_{1,sub}
\end{bmatrix}
=> =>
\begin{bmatrix}
A_{1,1} & A_{1,*} Q_{1,sub}\\
\begin{bmatrix}
1 \\ 0 \\ .. \\ 0
\end{bmatrix}
 & Q_{1,sub} A_{1,*} Q_{1,sub}
\end{bmatrix}
\end{equation}

As before we are looking to reflect the first column onto a coordinate axis so that we get zeroes. Just here we're leaving the diagonal untouched and reflecting the terms under it. So we want the *Q_{1,sub}A_{1,-}* column block matrix product to turn into *[ 1 0 0 0 .. 0 ]* and we can reuse ~first-column-reflector~  to get the appropriate *Q_{1,sub}*. Finally reusing ~raise rank~ we can build *Q_{1}*
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn hessenberg-form-first-partial-reflector
    "Builds a matrix that will reduce the first column of INPUT-MATRIX to  Hessenberg Form"
    [input-matrix]
    (if
        ;; Degenerate Case: 1 x 1 matrix
        (or (= (column-count input-matrix) 1) (= (row-count input-matrix) 1))
      [[ 1 ]]
    (let [first-column (get-column input-matrix 0)
          subdiagonal-column (subvector first-column 1 (dec (row-count first-column)))
          orthogonal-reducer (first-column-reflector subdiagonal-column)]
      (raise-rank orthogonal-reducer))))
#+END_SRC
 Once we've chose our *Q_{1}* we calculate the submatrix *Q_{1} A_{1,*} Q_{1}* and then call the recursively just like last time. Working back up the call stack the matrices are combined pretty much as before. We simply raise the rank of the *Q* matrices and pad the resulting matrices like we did for the resulting *R* matrices before.
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn hessenberg-form-reduction
    "Reduce the INPUT-MATRIX to  Hessenberg Form  - H , using reflectors - P. Result will be in the form [P^T H]"
  [input-matrix]
  (let [reflector-to-zero-out-first-column
        (hessenberg-form-first-partial-reflector input-matrix)
        input-matrix-with-first-column-zeroed-out
        (mmul reflector-to-zero-out-first-column input-matrix (transpose reflector-to-zero-out-first-column))]
    (if
        ;; Base Case: We're out of columns/rows to reduce
        ;;            Return the reflector and the reduced column
        (or (= (column-count input-matrix) 1) (= (row-count input-matrix) 1))
        [reflector-to-zero-out-first-column input-matrix-with-first-column-zeroed-out]
        ;; Recursive step: Get the Q^{-1}R of the submatrix
        ;;                 Then and combine it with your reflector and reduced matrix
        (let [submatrix (submatrix
                         input-matrix-with-first-column-zeroed-out
                         1 (dec (row-count input-matrix))
                         1 (dec (column-count input-matrix)))
              [submatrix-P submatrix-H] ( hessenberg-form-reduction submatrix)]
          [(mmul (raise-rank submatrix-P)
                 reflector-to-zero-out-first-column)
           (raise-rank-and-insert-row-column submatrix-H
                                     (subvector (get-column input-matrix-with-first-column-zeroed-out 0) 1 (dec (row-count input-matrix-with-first-column-zeroed-out)))
                                     (get-row input-matrix-with-first-column-zeroed-out 0))]))))
#+END_SRC
* TODOs
- add some TODOs
* SRC_Block template
#+BEGIN_SRC clojure :results output silent :session :tangle src/morelinear/core.clj
  (defn matrix-template
"template"
[matrix]
)
#+END_SRC

* End
#+BEGIN_Q^{-1}UOTE
This webpage is generated from an org-document (at ~./index.org~) that also generates all the files described. 

Once opened in Emacs:\\
- ~C-c C-e h h~ generates the webpage  \\
- ~C-c C-v C-t~ exports the code blocks into the appropriate files\\
- ~C-c C-c~     org-babel-execute-src-block
- ~C-c C-v C-b~ org-babel-execute-buffer
#+END_Q^{-1}UOTE
