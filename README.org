#+TITLE: MoreLinear \\ linear algebra methods in Clojure
#+DESCRIPTION: linear algebra methods in Clojure

#+EXPORT_FILE_NAME: index.html
#+HTML_DOCTYPE: html5
#+HTML_LINK_UP: ..
#+HTML_LINK_HOME: ..
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../web/worg.css" />
#+HTML_HEAD_EXTRA: <link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
#+HTML_MATHJAX: path: "../MathJax/MathJax.js?config=TeX-AMS_CHTML"
#+OPTIONS: html-style:nil
#+OPTIONS: num:nil
#+OPTIONS: html-postamble:nil
#+OPTIONS: html-scripts:nil

* Preface
This project developed out of an initial effort to build a working linear algebra system in ELisp called [[http://geokon-gh.github.io/elinear/index.html][~elinear~]]

As before I am following the textbook /Matrix Analysis and Applied Linear Algebra/ by Carl Meyer. I highly recommend it and I will reference it often, but this document won't depend on any information presented in ~elinear~ or the book.

Unlike [[http://geokon-gh.github.io/elinear/index.html][~elinear~]] this project doesn't explain Clojure nor the details of how to represent matrices on the computer. For more details, see the notes at the end.
* Linear systems : Ax=b
#+BEGIN_QUOTE
*Note:* The LU and GramSchmidt have been implemented in ELisp if you'd like to see how it's done: [[http://geokon-gh.github.io/elinear/index.html][elinear]]
#+END_QUOTE

#+BEGIN_QUOTE
*/Notes on getting a good numerical solution:/*

/Section 1.5/ goes into a good amount of detail of why floating point arithmetic will introduce errors and how to mitigate the problem. It's suggested to use *row-scaling* and *column-scaling*.

*Row-scaling* will alter the magnitude of the output value of your linear system, while *column scaling* will alter the scale of your inputs. Don't hesitate to have the inputs and outputs use different units. /page 28/ suggests scaling the rows such that the maximum magnitude of each row is equal to *1* (ie. divide the row by the largest coefficient)

The topic of /residuals/, /sensitivity/ and /coditioning/ will be revisited later
#+END_QUOTE
We start off with the most straightforward square system *Ax=b*. We are give a know square system *A* and a known output *B* and we are tasked with finding the corresponding input if it exists. We so far have the following methods:

- [[./gauss.html][*The LU decompositon*]] :: Also known as Gaussian reduction. Reduces our system *A* to two systems/matrices *L* and *U*. One lower triangular and the other upper triangular
- *The LU decompositon with partial pivoting* :: Similar to the previous method but we swap rows so that the pivot is always that largest column value.
- *The LU decompositon  with complete pivoting* :: Again similar to the previous method but the columns are switched as well.
- *The Gram Schmidt QR decomposition* :: Reduces the matrix *A* to an orthogonal basis *Q* and a upper triangular matrix *R*
- [[./householder.html][*The Householder QR decomposition*]] :: An alternate method to get a *QR* matrix pair through /Householder reflectors/
- *The Given QR decomposition* :: An alternate method to get a *QR* pair through rotation matrices

* Least Squares
The next step is extending the previous method to non-square systems and systems with no solution. In these cases applying the previous method directly will sometimes not work as a precise solution will not exist. We will need to use the [[./leastsquares.html][Least Squares method]] to find a nearby solution

* Similarity transforms : R^{-1}BRx=b
Exploring the posibilities of transforming our system *A* into a different basis where it has a more convenient form *B*. We take out inputs *x* from the standard basis into this new more convenient basis, then pass we them through the simpler system *B* and finally take the output and transform it back to the standard basis to get the output *b*.

- [[./hessenberg.html][*Hessenberg Form*]] :: Using /Householder reflectors/ we transform our matrix *A* into a form that is /almost/ upper triangular

* Other tools..
- *Cholesky Decomposition* :: Given a positive definite matrix (one that is symmetrix and has positive values on the diagonal) after we do Gaussian reduction we get *A=LDU* (where *L* and *U* have *1*'s on the diagonal and *D* is unit diagonal. Since *A* is symmetric then *A=A^{T}* and therefore *LDU=(LDU)^{T}=U^{T}D^{T}L^{T}*. Since *D==D^{T}* and the *LU* factorization is unique then *L* must equal *U^{T}* and *U* is equal to *L^{T}* - so we can write *A=LDL^{T}*. Then we can split *D* but taking the square root of the diagonal elements (that's why they need to be positive!) and get *A=LD^{1/2}D^{1/2}L^{T}*. set *R=LD^{1/2}* and you can write *A=RR^{T}*

- [[./vandermonde.html][*Vandermonder Matrices*]] :: These matrices are used to solve for a polynomials equation /f(x)/ such that it fits a list of =x/y= coordinates. This is especially useful in combination with the Least Squares method to fit approximate polynomials
* Notes on Clojure
This new linear algebra project is written in Clojure - a language that runs on both the JVM and in Javascript (I'm only really testing things on the JVM at the moment). There should be no language specific funny buisness and though everything is written in Clojure's "functional style" it shouldn't be difficult to translate to a different language and library

** ~core.matrix~
In an effort to mitigate the issue I had in ELisp, I'm using the ~core.matrix~ library which acts as a "front end" API for many different backends - some on the JVM others in JS. It provides lots of helper functions so I can quickly write what I want. The library is generally very flexible and full features and uses a very generic N-dimensional array system.

That said, it's also not extremely performant and you can very easily end up doing operations that are very slow. There are lot of cases that this library is simply not set up to handle in an intelligent way and the N-dimensional array paradigm is in many partical scenarios a hinderance.

#+BEGIN_QUOTE
For instance if you want to represent a convolution using matrices you would take you input signal (which in for instance a short audio clip will equal to ten of thousands of data points) and mutliply it by a band matrix that is =N=x=N=. In a N-dimensional dense matrix system such a multiplication is either impossible or exceedingly slow. In a more advance matrix system you would have a special band matrix object and special matrix mutliplication operators for it that would be very efficient.
#+END_QUOTE

A more full featured performant library will start mixing in band matrices, symmetric matrices, upper/lower diagonal matrices.. etc etc and so the whole system becomes quite complicated and interdependent - and for the education purposes of this project that's mostly just noise

If you are concerned about getting as much as you can out of your system then I suggest looking at the ~neanderthal~ library which provides a thin wrapper around the Intel MKL (which in effect BLAS). It makes it very easy to work with BLAS and if you're on an x64 system this is more or less the best CPU based performance you can hope for really. 

#+BEGIN_QUOTE
*Note*: This project has a ~neanderthal~ branch where I starter implementing a few of the first functions. It should give you a good taste of what working in the more constrained BLAS environment looks like.

  It's an interesting work flow and really forces you to think a lot harder about your algorithms. But there is more code noise and the helper function need to be written manually for each case (b/c each one has its own nuances). It also doesn't run on ARM or the browser. It does have the ability to run in OpenCL but this funcationality also strangely requires an x64 system to back it up (for passing the matrices to and from the GPU)

#+END_QUOTE

** Project managment
Project management in Clojure is done through a top level ~deps.edn~ file which specifies project dependencies . In our case it's ~core.matrix~ and the ~vectorz~ backend (without which some functionality is broken).
#+BEGIN_SRC clojure :results output silent :session :tangle deps.edn
  {:deps
   {net.mikera/core.matrix {:mvn/version "0.62.0"}
    net.mikera/vectorz-clj {:mvn/version "0.48.0"}}}
#+END_SRC

* TODOs
- Is a lower triangular matrix orthogonal? Is L and orthogonal basis for *U* in *LU*?
- Why does the jumbo matrix for the *LU* matrix work? (see least squares)
- Schur decomposition/compliment
- Implement the Sherman-Morrison update formula
- Sensitivity/Condition numbers needs to be revisited and expanded on (page 126-128)
- Do exercise 3.8.8
- Tridiagonal matrices - 3.10.6
- Implement the Least Squares numerical stability comparison (and maybe speed tests as well)
- Maybe work out a motivational exercise to drive all this..
- Figure out why the Cholesky doesn't need a permutation: https://math.stackexchange.com/questions/621045/why-does-cholesky-decomposition-exist
* End
#+BEGIN_QUOTE
This webpage is generated from an org-document (at ~./index.org~) that also generates all the files described. 

Once opened in Emacs:\\
- ~C-c C-e h h~ generates the webpage  \\
- ~C-c C-v C-t~ exports the code blocks into the appropriate files\\
- ~C-c C-c~     org-babel-execute-src-block
- ~C-c C-v C-b~ org-babel-execute-buffer
#+END_QUOTE
