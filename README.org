#+TITLE: Linear Systems - Part 2:  Clojure
#+DESCRIPTION: Some linear algebra in Clojure

#+EXPORT_FILE_NAME: index.html
#+HTML_DOCTYPE: html5
#+HTML_LINK_UP: ..
#+HTML_LINK_HOME: ..
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../web/worg.css" />
#+HTML_HEAD_EXTRA: <link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
#+HTML_MATHJAX: path: "../MathJax/MathJax.js?config=TeX-AMS_CHTML"
#+OPTIONS: html-style:nil
#+OPTIONS: num:nil

* Preface
This is a continuation (with some overlap) of what I have been developing in [[http://geokon-gh.github.io/linearsystems-part1/index.html][Part 1]]. There I had developed a linear algebra system from scratch in ELisp and showed how to use it in several different fundamental applications. However as the algorithms become more complicated, the code started to get a little out of hand. A larger fraction of the time and code was spent on helper functions and extending the system to support different operations I needed and certain design simplifications and errors I had made at the beginning made the ultimate system inflexible and a bit unsatifying to work with.

** Neanderthal
To carry on working a bit less encumbered with my own mistakes and to push my comfort zone a bit, I've switched over to Clojure and the ~neanderthal~ library. The library provides a wrapper for the Intel ~BLAS~ library called ~Intel MKL~. It's probably the most optimized solution out there for running linear algebra on the CPU. If you look at [[https://neanderthal.uncomplicate.org/codox/uncomplicate.neanderthal.core.html][the documentation]], some additional functionality that I was missing is now available, but at the same time lots of little conveniences are missing. Typically this means that these features have performance issues and so we will need to look for alternatives.

#+BEGIN_QUOTE
*Note* I don't /actually/ know how to use BLAS/Neanderthal properly and I'm winging it and figuring things out as I go along. If there are mistakes please please make an issue
#+END_QUOTE

As before, this is an org document and can be tangled into the full Clojure project without the need for any external files. The tangled output will also be tracked in the repository, (but is not necessary if you use Emacs)

Explaining Clojure is outside the scope of this project, but in short you will need to install Java and [[http://leiningen.org/][Leiningen]] and the intel MKL (on Ubuntu 18.10 and higher this is a lot easier b/c it's all available in the offical repositories). After you have both installed, you just clone this repository, go into it, and run ~lein run~ to run the project or ~lein repl~ to start an interactive REPL session.

* Project managment
Project management in Clojure is done through a top level =leiningen= file ~project.clj~  which specifies dependencies and other details.
#+BEGIN_SRC clojure :results output silent :session :tangle project.clj
  (defproject linearsystems-part2 "0.1.0-SNAPSHOT"
    :description "linea-systems in Clojure"
    :url "http://geokon-gh.github.io/linearsystems-part2/index.html"
    :license {:name "Eclipse Public License"
	      :url "http://www.eclipse.org/legal/epl-v10.html"}
    :dependencies [[org.clojure/clojure "1.10.0"]
		   [uncomplicate/neanderthal "0.22.0"]]
    :main ^:skip-aot linearsystems-part2.core
    :target-path "target/%s"
    :profiles {:uberjar {:aot :all}})

#+END_SRC
The rest of the code will live in ~src/core.clj~ as is the convention (maybe if there is a lot of code or parts to break off, these will be in separate namespaces/files..)
We start by declaring the project namespace and including all of ~neanderthal~. Since we will be using it extensively and I don't want to overload any of its names, I'm not even bothering to alias it.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (ns linearsystems-part2.core
    (:use [uncomplicate.neanderthal core native])
    (:gen-class))

  (defn -main
    "I don't do a whole lot ... yet."
    [& args]
    (println "Hello, World!"))

#+END_SRC
The ~-main~ is just a placeholder for the moment. With the setup out of the way, we can move on to the actual matrix magic

* Householder QR
An alternate method to the Gram-Schmidt is to take a more direct approach to building a *QR* matrix - similar to how we worked on building the *LU*. We will just directly zero out columns to build an upper triangular *R*. The difference from the *LU* is that now instead of using elementary matrices to do row operations to get zeroes we will restrict ourselves to using *elementary reflectors*. Their key property is that they are orthonormal, so when we carry out a series of reflections *Q_{1}Q_{2}..Q_{k}A* we can combine them into one matrix which will be guaranteed to be orthonormal as well. Conceptually this is saying that if you do a bunch of reflections one after another, it all adds up in the end to be one reflection - and this makes some intuitive sense. In the *LU*'s Gaussian elimination the elementary matrices we used were neither orthogonal nor normal and we didn't have this same guarantee that the product of several elementary matrices gives another elementary matrix.


** elementary reflector

An /elementary reflector/ does what's written on the label, it's a matrix/linear-system which when given a vector produces its reflection across a /N-1/ dimensional hyperplane. In the next section we will deal with selecting the correct hyperplane, but for the time being we will just focus on building such a matrix. 

The first task is finding a nice concise mechanism to define a hyperplane. If our space is *R^N* then the hyperplane will be *N-1* dimensions and at first blush we seem to need *N-1* vectors to define it. For instance in *3D* space any two vectors not on the same line will define a *2D*. Where *plane = a*v_1 + b*v_2* /for all a and b/. But this method doesn't really scale b/c as *N* increases so does the number of vectors you need. The shortcut is that actually all planes have vectors orthogonal to the hyper plane. These vectors all lie on the same line and we just choose one, call it *u*, and let it represent that remaining *N^{th}* dimension. Now you can simply say that the hyperplane is all vectors orthogonal to *u*. Or more formally, all vectors /not-in-the-span/ of *u* are the hyperplane

Now that we have a way to define a plane we need to work through the mechanics of relfecting an arbitrary vector *x* across the hyperplane. The key insight here is that *x* can be treated as two separate vectors. One that lies in the plane and one that is orthogonal to the plane. The component that lies in the plane is unaffected by the reflection while the component that is orthogonal is basically flipped to point to the other side of the plane. To do this procedure mathematically we take the component of *x* in the direction of the *u* that defines our plane and then we subtract it twice from *x*. This will give us a new vector that points at its own reflection on the other side of the plane. Breaking it down further, the steps go as followed:
 -  *u^{t}x*/||u||* is the amount of *x* in the direction of *u* (a scalar)
 -  *uu^{t}x/||u||^{2}* is the component *x* in the direction of *u* (a vector)
 -  Here we notice that we can subsitute the inner product *u^{t}u* for *||u||^2*
 -  *uu^{t}x/u^{t}u*
 -  *x - 2uu^{t}x/u^{t}u* is you subtracting that vector component twice to get its reflection
 -  *(I-2uu^{t}/u^{t}u)x* is how we'd factor the *x* back out
 -  *(I-2uu^{t}/u^{t}u)* represents the reflector matrix. It's defined uniquely by the *u* vector!
Note that now that we factored out this matrix that given any new value of *x* we can multiply it by *(I-2uu^{t}/u^{t}u)* and get its reflection across the hyperplane orthogonal to *u*. 

*Note:* We will see in the next section that we can't safely assume *u* is unit length - otherwise we could drop the normalizing inner product term and everything would look much cleaner.

*** the code
This will be our first foray into working with Intel BLAS and neanderthal and it's good to have a big picture understanding of how working with this library will be quite different from working in ELisp. First of all we will need to be a lot more conscious of where our data is and we will need to make more effort in avoiding any needless copying. The available functions are split into several general categories. There are functions that change data in place (they end in an ~!~), there are generally identical functions that will copy the result into a new matrix/vector/etc. and finally there are functions that don't copy or write anything but simply return a new interface to the same underlying data.

Next, unlike in MATLAB, vectors are not treated just as column matrices as you see in most literature. At first this is a bit annoying, but the rational is sound and a good explanation of this design decision is here: https://github.com/mikera/core.matrix/wiki/Vectors-vs.-matrices and here: https://groups.google.com/d/topic/numerical-clojure/zebBCa68eTw/discussion

To start we need to make an identity matrix. For some reason this isn't part of =Neanderthal= nor =IntelMKL=. I'm not quite sure why, maybe because depending on your use-case you will want to put it in a different matrix container (triangular, dense, symmetrical etc.). In any case.. for our usecase we will just use the diagonal. If this becomes problematic later we can revisit this:
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn identity-matrix
    "Create an identity matrix of DIMENSION"
    [dimension]
    (entry! (dgd dimension) 1))
#+END_SRC
 - ~dgd~ creates a /diagonal matrix/ (this is a more compact matrix representation than a full dense matrix)
 - ~entry!~ set the values in the matrix to some value

Next we will need to add a ~self-outer-product~ function to calculate the *uu^{t}* in our reflection matrix equation. At first it seems like there doesn't existing a function for this =neaderthal= or =Intel BLAS= - but it's actually kinda hidden in the ~(rk ..)~. This the *Rank 1 Update* function.. which is used to update *QR* decompositions when we get new data.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn self-outer-product
    "Returns the outer product of a vector with itself"
    [input-vector]
    (view-sy (rk input-vector
		 input-vector)))
#+END_SRC

We have an added guarantee that the result with be symmetric b/c *(uu^{t})^{t}=uu^{t}* so we can wrap the output in a ~view-sy~ which will make the result show up as a symmetric matrix.

Now that we have all the pieces we need we write out the equation for the elementary reflector. Give a *u* it'll return a reflector matrix.

#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn elementary-reflector
    "Build a matrix that will reflect vectors across the hyperplane orthogonal to REFLECTION-AXIS"
    [reflection-axis]
    (let [outer-product-matrix (self-outer-product reflection-axis)]
      (axpy!
       (dia (identity-matrix (dim reflection-axis)))
       (dia (scal! (/ -2 (dot reflection-axis reflection-axis))
		   outer-product-matrix)))
      outer-product-matrix))
#+END_SRC

 - ~dim~ returns the number of elements in the input vector
 - ~axpy!~ in an in-place addition where the values are added "in-place" to the second matrix/vector. Here we're carrying out the subtraction step in *(I-2uu^{t}/u^{t}u)*
 - ~dia~ makes the diagonal elements of a matrix look like a vector. This is just a new view to the same underlying data in the input matrix so that even though we're adding 2 sectors it's actually adding the diagonal elements of the matrices under the hood.
 - ~scal!~ is an in-place multiplication of a matrix by a scalar. Here the scalar is *-2/u^{t}u* and the matrix is the outer product *uu^{t}*.

For example:
#+BEGIN_SRC clojure
  (elementary-reflector (dv [43.0 36.0 38.0 90.0]))

  ;; #RealUploMatrix[double, type:sy, mxn:4x4, layout:column, offset:0]
  ;;    ▥       ↓       ↓       ↓       ↓       ┓    
  ;;    →       0.71    *       *       *            
  ;;    →      -0.24    0.80    *       *            
  ;;    →      -0.26   -0.22    0.77    *            
  ;;    →      -0.61   -0.51   -0.54   -0.28         
  ;;    ┗                                       ┛   


  (mm (elementary-reflector (dv [43.0 36.0 38.0 90.0]))
    (dge 4
	 1
	 (dv [43.0 36.0 38.0 90.0])))

  ;; #RealGEMatrix[double, mxn:4x1, layout:column, offset:0]
  ;;    ▥       ↓       ┓    
  ;;    →     -43.00         
  ;;    →     -36.00         
  ;;    →     -38.00         
  ;;    →     -90.00         
  ;;    ┗               ┛  
#+END_SRC

This is subtle, but important: Notice how the output of ~(elementary-reflector .. )~ shows up as =#RealUploMatrix[..]= ie. as a symmetric matrix. Neanderthal has cascaded the original symmetric matrix /type/ from ~self-outer-product~ all the way through all the operations we did b/c it knows they didn't change the symmetry. This is very powerful and useful! You can just imagine having to juggle the types in C/C++ .. 

*TODO* - Include Plot

** elementary coordinate reflector
Circling back to our original intent, we were trying to use reflectors to clear rows and build an upper triangular matrix (the *Q* in the *QR*). 

The first thing we want to do is have a method to zero out the first column of a matrix, ie *A_{ - ,1}*, If we could build a special elementary reflector *Q_{1}* that reflected that first column on to the elementary vector *e_{1}* (that's *[ 1 0 0 0.. 0 ]* ), then *Q_{1}A* would leave everything under the the first column zeroed out

Generalizing a bit further, we will write a function that take a given vector and a target coordinate axis and produce a reflection matrix that takes one to the other. This is a bit of an inversion of what we did in the previous section. Instead of taking a hyperplane and reflecting over it, we now know what we want to reflect and where we want to reflect it to - we just need to find the right plane to do it. The answer isn't all that complicated, but it's a bit hard to picture. We want to find the plane that lies between where we start and where we want to reflect to. If you have two vectors and you want to make a plane that goes right between them, then all you need to do is make the two vectors the same length and add them together. This will give you a vector that goes right between the two. If you picture it in 2D space then the two vectors add up into a diamond shape with the point lieing on the bisecting line. 

The last catch is that to define out plane we actually want the orthogonal vector. However, if instead you subtract these two equal-length vectors, then you will find that you get a vector orthogonal to that bisection vector/plane.

\begin{equation}
u = x - ||x||e_{1}
\end{equation}

Once you have the orthogonal vector to the bisecting plane, you just feed it into our previous function and get the reflection matrix!
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn elementary-coordinate-reflector
    "Build a matrix that will reflect the INPUT-VECTOR on to the COORDINATE-AXIS"
    [input-vector coordinate-axis]
    (scal! (nrm2 input-vector) coordinate-axis)   ; scale coordinate axis
    (if (= input-vector coordinate-axis) ; degenerate case
      (identity-matrix (dim input-vector))                           ; return identity matrix
      (elementary-reflector (axpy -1                      ; make input-vector orthogonal to the bisecting plane
				  coordinate-axis
				  input-vector))))
#+END_SRC
For instance we can take some random vector and say we want to reflect it onto the *e_1*
#+BEGIN_SRC clojure
  (elementary-coordinate-reflector (dv [24 77 89 12]) (dv [1 0 0 0]))
  ;; #RealUploMatrix[double, type:sy, mxn:4x4, layout:column, offset:0]
  ;;    ▥       ↓       ↓       ↓       ↓       ┓    
  ;;    →       0.20    *       *       *            
  ;;    →       0.64    0.49    *       *            
  ;;    →       0.74   -0.59    0.32    *            
  ;;    →       0.10   -0.08   -0.09    0.99         
  ;;    ┗                                       ┛    
#+END_SRC
We got some seemingly random matrix out. If we then multiply it times out random vector, it reflects perfectly to *e_1*
#+BEGIN_SRC clojure
  (mm (elementary-coordinate-reflector (dv [24 77 89 12])
				       (dv [1 0 0 0]))
      (dge 4
	   1
	   (dv [24 77 89 12])))
  ;; #RealGEMatrix[double, mxn:4x1, layout:column, offset:0]
  ;;    ▥       ↓       ┓    
  ;;    →     120.71         
  ;;    →      -0.00         
  ;;    →      -0.00         
  ;;    →      -0.00         
  ;;    ┗               ┛    
#+END_SRC

** Zeroing the first column

Now putting all the pieces together, given some matrix *A* we can get back a reflector to zero out its first column
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn elementary-vector
    "Make an elemntary vector of INDEX and DIMENSION"
    [index dimension]
    (dv (assoc (into [] (repeat dimension 0)) index 1)))

  (defn first-column-reflector
    "Build a matrix that will reflect the INPUT-MATRIX such that the first column end up on [ 1 0 0 .. 0 ]"
    [input-matrix]
    (elementary-coordinate-reflector (col input-matrix 0)
				     (elementary-vector 0 (mrows input-matrix))))

  ;;(dv (assoc (into [] (repeat (mrows input-matrix) 0)) 0 1))
#+END_SRC

Now we can test it by writing out a random matrix and zeroing out its first column
#+BEGIN_SRC clojure
  (first-column-reflector (dge [[43.0 36.0 38.0 90.0]
				[21.0 98.0 55.0 48.0]
				[72.0 13.0 98.0 12.0]
				[28.0 38.0 73.0 20.0]]))

  ;; #RealUploMatrix[double, type:sy, mxn:4x4, layout:column, offset:0]
  ;;    ▥       ↓       ↓       ↓       ↓       ┓    
  ;;    →       0.47    *       *       *            
  ;;    →       0.23    0.90    *       *            
  ;;    →       0.79   -0.35   -0.19    *            
  ;;    →       0.31   -0.14   -0.46    0.82         
  ;;    ┗                                       ┛  

  (let [A (dge [[43.0 36.0 38.0 90.0]
		[21.0 98.0 55.0 48.0]
		[72.0 13.0 98.0 12.0]
		[28.0 38.0 73.0 20.0]])]
    (mm (first-column-reflector A)
	A))
  ;; #RealGEMatrix[double, mxn:4x4, layout:column, offset:0]
  ;;    ▥       ↓       ↓       ↓       ↓       ┓    
  ;;    →      90.87   61.69  130.83   69.35         
  ;;    →      -0.00   86.73   14.28   57.06         
  ;;    →      -0.00  -25.64  -41.61   43.06         
  ;;    →       0.00   22.97   18.71   32.08         
  ;;    ┗                                       ┛    
#+END_SRC

Now we hit a bit of a snag. You can use the same method to make some matrix *Q_2* that will zero out the second column, but when you combine the two and try *Q_2Q_1A* you will see that *Q_2* is messing up the first column - so we lose the progress we'd made in the first step. We may have gotten the first column to lie on the coordinate vector after *Q_1A*, but when you reflect it again it moves away from the coordinate vector.

In the *LU* Gaussian Elimination method we didn't have this problem b/c clearing subsequent columns was guaranteed to leave you previous columns intact (it would just shuffling zeroes around). Now this guarantee is gone so we need to think of a new trick.

** Zeroing out the second column

As is usually the trick with these things, the solution is thinking in terms of block matrices. When we say we need to clear the second column we actually mean taking the result of our *Q_1A* and clearing everything under the =(2,2)= position. To approach this we construct a new block matrix for *Q_2* that will have the form

 \begin{equation}
 Q_{2}
 =
 \begin{bmatrix}
 1 & 0\\
 0 & S_{ n-1, m-1 }\\
 \end{bmatrix}
 \end{equation}

Notice how when we multiply this matrix times *Q_1A* the first column is left untouched and this =(n-1,m-1)= submatrix *S* will multiple times a submatrix of *Q_1A* which has the =(2,2)= position now in the =(1,1)= position.


 \begin{equation}
 Q_2(Q_1A)
 =
 \begin{bmatrix}
 1 & 0\\
 0 & S\\
 \end{bmatrix}
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & (Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 =
 \begin{bmatrix}
 (Q_{1}A)_{1,1} & (Q_{1}A)_{1,*}\\
 0 & S(Q_{1}A)_{n-1,m-1}\\
 \end{bmatrix}
 \end{equation}

In the resulting matrix the only "new" entry we need to worry about is *S(Q_{1}A)_{n-1,m-1}* - everything else is unchanged. In this submatrix we need to clear the first column because it's the second column of our overall matrix - and we are free to choose any appropriate *S* matrix to do it. At this point the problem of choosing *S* mirrors the process we used to clear the first column - the only difference being that the dimension is one smaller

When tackling the third column we just repeat the process but starting with the *S(Q_{1}A)_{n-1,m-1}* matrix - so the method is starting to show some recursion

*** Reducing to R in code

A recursive solution is already very promising, however thanks to BLAS and =neanderthal= we can actually go one step further because their APIs allow us to reference blocks using the ~(submatrix ..)~ function. This gives us a new matrixobject that we can manipulate on its own, but its underlying data will remain tied to the matrix we pulled it out of. So if we start modifying the submatrix its larger parent matrix will also change in turn

At each iteration of the algorithm, like when generating *Q_{2}Q_{1}A* we will want to grab the submatrix from the preceeding input matrix - so *(Q_{1}A)_{n-1,m-1}* from *Q_{1}A* in our example above - and we will want to "modify"/reflect this submatrix with *S* and then repeate the process with the next immediate submatrix. Looking at our *Q_{2}Q_{1}A* example we can already see that the rest of *Q_{1}A* won't need to be changed - we just need to relfect that submatrix and that's it.
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn reduce-to-r
    "Reduce a matrix to a lower triangular orthonormal matrix"
    [input-matrix]
    (if (= 1 (dim input-matrix)) ;; base case - 1x1 matrix .. nothing to reduce
      (input-matrix)
      (do (mm! (first-column-reflector input-matrix)
	       input-matrix)
	  (recur (submatrix input-matrix
			     1
			     1
			     (dec (mrows input-matrix))
			     (dec(ncols input-matrix)))))))

#+END_SRC

*WIP*: The above code doesn't work b/c unfortunately you can't ~mm!~ in place with a [[https://github.com/uncomplicate/neanderthal/issues/67][symmetric matrix]]. So the algorithm needs to be reworked. It's unclear if it can be made to work in-place with no copying.
#+BEGIN_SRC clojure
(let [A (dge [[43.0 36.0 38.0 90.0]
		[21.0 98.0 55.0 48.0]
		[72.0 13.0 98.0 12.0]
		[28.0 38.0 73.0 20.0]])]
    (reduce-to-r A))

(let [A (dge [[43.0 36.0 38.0 90.0]
		[21.0 98.0 55.0 48.0]
		[72.0 13.0 98.0 12.0]
		[28.0 38.0 73.0 20.0]])]
    (mm! (first-column-reflector A)
	 A)



#+END_SRC

This process continues recursively to the third column and so on.. until we hit the 2x2 case and the whole matrix has been zeroed out.

* TODOs
- add some TODOs
* SRC_Block template
#+BEGIN_SRC clojure :results output silent :session :tangle src/linearsystems_part2/core.clj
  (defn matrix-template
"template"
[matrix]
)
#+END_SRC

* End
#+BEGIN_Q^{-1}UOTE
This webpage is generated from an org-document (at ~./index.org~) that also generates all the files described. 

Once opened in Emacs:\\
- ~C-c C-e h h~ generates the webpage  \\
- ~C-c C-v C-t~ exports the code blocks into the appropriate files\\
- ~C-c C-c~     org-babel-execute-src-block
- ~C-c C-v C-b~ org-babel-execute-buffer
#+END_Q^{-1}UOTE
