#+TITLE: MoreLinear \\ Fourier
#+DESCRIPTION: linear algebra methods in Clojure

#+INCLUDE: "../web/config.org"

* Goal
Using the multiplicative properties of complex numbers we construct an easily-invertible orthogonal basis in which periodic components of our input become apparant. This basis allows us to carry out a class of operations that are numerically intensive in the standard basis but much quicker in the frequency-basis.

* Complex numbers
Complex numbers are of the form *a+ib* where *i= \radic(-1)* and *a* and *b* are known as the real and imaginary components of the complex number. The pair (a,b) are often referred to as a point on the complex plane. However the implicit analogy to 2D coordinates is a bit probematic

To envision a complex plane we treat the /real/ component *a* acting as the /x/ and the /imaginary/ component *b* acting as the /y/.
Like 2D coordinates, we can add/translate them and they can be scaled
 - (a+ib) + (c+id) = (a+c) + i(bd)
 - c * (a+ib) = ca +icb 

However unlike 2D [x,y] coordinates, complex numbers can be multiplied:
 - (a+ib) * (c+id) = (ac-bd) + i(ad+bc)

Normally if we are given 2 coordinates [x_{0},y_{0}] and [x_{1},y_{1}] we never ask ourselves to multiple them because multiplication between two points is simply no a defined operation. However in the complex-number case this operation comes naturally and it generates a new complex number

#+BEGIN_QUOTE
Note: You can do a dot and cross product between two [x,y] coordinates but:
 - [x_{0},y_{0}] \bullet [x_{1},y_{1}] \to /scalar/
 - [x_{0},y_{0}] \times [x_{1},y_{1}] \to /requires an extra =z= dimension/
#+END_QUOTE
 
So points in the complex plane have this extra "feature" so it's important to not stretch the 2D coordinate interpretation too far. As far as I can tell there is no easy way to visualize what the result of a multiplication looks like in the general case.

* Complex Conjugates
Just like points and vectors, complex numbers have a "length" or magnitude
 - ||a+ib|| = \radic(a^{2}+b^{2}}
And just like for points and vectors, this norm is a real scalar value

The square of the norm is ofcourse just /a^{2}+b^{2}/ and this often comes up in things like the inner product.  For instance given a vector [x y] we can take its inner product with itself [x y][x y]^{T} to get its norm-squared. In the trivial =1x1= case this will be [z][z]^{T} which is z^{2}.

In the complex case where /z/ is a complex number /a+ib/ then we'd get /z*z^{T}=z^{2}=(a+ib)^{2}=a^{2}+i2ab-b^{2}/ which is non-real and doesn't match our magnitude-squared. So using the transpose no longer works. But this norm-squared is still easy to express as a product
 - a^{2}+b^{2} = (a+ib)(a-ib)
With /a-ib/ known as the /complex conjugate/ of /a+ib/

And what's convenient is that this actually still works for real values. Since a real value /a/ can be written as the complex number a+i*0 we can write its complex conjugate as a-i*0 and see that it's norm is
 - a^2+0^2 = (a+i0)(a-i0) = a^2
Which is just the absolute value of /a/ squared

So instead of doing a transpose we need to introduce a new transposition called the Hermitian transpose
 - [a+ib c+id]^{*} = [a-ib c-d]^{T}
The Hermitian transpose is denoted with a star. It simply take a normal transpose and replaces the terms with their complex conjugates.

 When the vector values are all real it will behave exactly like a transpose and when the values are complex, it will take their conjugates and give us scalar/real values of each complex values' magnitude

* The Unit Circle
Notice that when the /norm-squared/ is equal to /1/ the magnitude is also equal to /1/. All these special points who's length is equal to /1/ lie on what's called the /unit cirle/. If we start with two complex numbers /a+ib/ and /c+id/ then
 - (a^2+b^2)^{1/2} = 1 /and therefore/ a^2+b^2 = 1
 - (c^2+d^2)^{1/2} = 1 /and therefore/ c^2+d^2 = 1
As we saw, their product is the point /(ac-bd) + i(ad+bc)/ and its length is also equal to 1. Proof:
 - [ (ac-bd)^{2} + (ad+bc)^{2} ]^(1/2)
 - [ a^{2}c^{2}-2acbd+b^{2}d^{2}+a^{2}d^{2}+2adbc+b^{2}c^{2} ]^{1/2}
 - [ a^{2}c^{2}+b^{2}b^{2}+a^{2}d^{d}+b^{2}c^{2} ]^{1/2}
 - [ c^2(a^2+b^2) + d^2(a^2+b^2) ]^(1/2)
Through some subsitution we see that the length of the product is also equal to /1/

This tells us that while the general complex product is hard to think about, we know that least the product of any two points on the unit circle gives us a new point on the unit circle

* Euler's Formula

Combining the definition of /sine/ and /cosine/ with pythagoras' theorem (for the case where the hypotenous is equal to one) gives us the trigonometric identity:
 - cos^{2}(\theta)+sin^{2}(\theta) = 1

And then Euler's formula tells us that: *(TODO: Exapand on this)*
 - e^{i\theta}=cos(\theta)+isin(\theta)

Combining these two statements allows us to safely say that the magnitude of the complex value /e^{i\theta}/ is always equal to /1/. In fact, every point on the complex-plane unit circle corresponds to a euler exponent. Now using what we know of the product of points on the unit circle we can say that
 -  || e^{i\theta} * e^{i\omega} || = 1
You can also just add the exponents  /e^{i(\theta+\omega)}/ and reusing the trigonometric identity.

Now notice that if /\omega=2\pi-\theta/ that:
 - e^{i(\theta+\omega)} = e^{i2\pi} = cos(2\pi) + isin(2\pi) = 1 + i0 = 1

Also note that complex conjugate of 
 - e^{i\theta}=cos(\theta)+isin(\theta)
is
 - cos(\theta)-isin(\theta)
Using the trigonometric properties (which are self evident if you think of the graph and even/odd functions)
 - sin(-\theta)=-sin(\theta)
 - cos(-\theta)= cos(\theta)
We can rewrite the conjugate as
 - cos(-\theta) + isin{-\theta) = e^{-\theta}

* The roots of unity

Furthermore if /\alpha=2\pi/n/ then:
 - [e^{i\alpha}]^{n} = e^{in\alpha} = e^{i2\pi} = 1
This tells us that taking the n^{th} root of /1/ has a complex numbers solution! (in addition to the trivial solution of /1/)
 - 1^{1/n} = e^{i2\pi/n}
The typical notation here is to say 
 - \xi = e^{-i2\pi/n} = \cos(2\pi/n)+i\sin(2\pi/n)
 - \xi^{n}=1
This \xi is called the *n^{th} root of unity*
By extension we can also see that:
 - \xi^{n+j} = \xi^{n}\xi^{j} = \xi^{j}
 - \xi^{nk} = [\xi^{n}]^{k} = [e^{-i2\pi}]^k = 1^k = 1

* Fourier series

The Fourier series is a very special sum of the exponents of an n^{th} root of unity *\xi*
 -  1 + \xi^k + \xi^{2k} + ... + \xi^{(n-2)k} + \xi^{(n-1)k}
Here the /k/ can be any integer value, but the sum will always maintain the property that if it's multiplied by /\xi^k/ we get the same sequence back. The last term goes to /\xi^{nk} = 1/ and the remaining terms in effect shift places giving us:
 -  \xi^k + \xi^{2k} + \xi^{3k} + ... + \xi^{(n-1)k} + 1
So we can write
 - \xi^k * /fourier-series/ = /fourier-series/
Therefore
 - \xi^k * /fourier-series/ - /fourier-series/ = 0
 - /fourier-series/ * (\xi^k - 1) = 0
And therefore.. 
 - /fourier-series/  = 0
.. or to write it out again in full form - for all integer values of /k/
 -  1+\xi^k+\xi^{2k}+...+\xi^{(n-2)k}+\xi^{(n-1)k} = 0

* Fourier Vectors

It turns out that Fourier series, when places in a vector with different values of /k/, form mutually orthogonal vectors - here I choose /r/ and /s/ for /k/ and carry out the Hermitian inner product:
 - [ 1 \xi^r \xi^{2r} ... \xi^{(n-1)r} ] [ 1 \xi^s \xi^{2s} ... \xi^{(n-1)s} ]^{*}
 - [ 1 \xi^r \xi^{2r} ... \xi^{(n-1)r} ] [ 1 \xi^{-s} \xi^{-2s} ... \xi^{-(n-1)s} ]^{T}
 - 1*1 + \xi^r*\xi^-s + \xi^{2r}*\xi^{-2s} + ... + \xi^{(n-2)r}\xi^{-(n-2)s}} + \xi^{(n-1)r}\xi^{-(n-1)s}
 - 1 + \xi^{r-s} + \xi^{2(r-s)} + ... + \xi^{(n-2)(r-s)} + \xi^{(n-1)(r-s)}
We can factor out the (r-s) exponent *TODO*. FIX. THIS ISN"T CORRECT
 - (1^{1/(r-s)} + \xi^{1} + \xi^{2} + ... + \xi^{n-2} + \xi^{n-1})^{r-s}
 - (1 + \xi^{1} + \xi^{2} + ... + \xi^{n-2} + \xi^{n-1})^{r-s}
And notice that the sum under the exponent is the just fourier series with /k=1/ and therefore equal to /0/. So the inner product of two Fourier series is always equal to zero *except* when /r=s/:
 - [ 1 \xi^k \xi^{2k} ... \xi^{(n-1)k} ]  [ 1 \xi^k \xi^{2k} ... \xi^{(n-1)k} ]^{*}
 - 1*1 + \xi^{k}\xi^{-k} + \xi^{2k}\xi^{-2k} + ... + \xi^{(n-1)k} \xi^{-(n-1)k}
 - 1*1 + \xi^{0} + \xi^{0} + ... + \xi^{0}
 - 1 + 1 + 1 + ... + 1 = n
You could also just view the /r=s/ case as the sum of the 2-norms of the roots of unity. There are /n/ terms and each one is necessarily of length /1/. Either way, the answer will always be /n/ no matter what /k/ you choose
and therefore the 2-norm/length all all fourier vectors is /n^{1/2}/

It's important to note that if you drop all the complex terms none of this works! You can't construct mutually orthogonal vectors out of purely real sine/cosine waves

* Fourier Matrix

We've just shown that the fourier vectors are orthogonal as long as the /k/'s are different. So the next step is self evident. We just choose /k/ to equal to 1 through /n/ and slap them together into a fourier matrix *F* and get ourselves an orthogonal basis:\\
*F* =
| 1 | 1         | 1         | 1     | .. |
| 1 | \xi       | \xi^2     | \xi^3 | .. |
| 1 | \xi^2     | \xi^4     | \xi^6 | .. |
| 1 | \xi^3     | \xi^6     | \xi^9 | .. |
| 1 | ...       | ...       | ...   | .. |
| 1 | \xi^{n-1} | \xi^{n-2} | ...   | .. |

Looking at the real components of the columns - there is one constant component (the first column) and then /n/ samples of the /cosine/ function over one oscillation for the second column, /n/ samples over 2 periods for the 3rd column, /n/ samples over 3 periods.. etc. \\
\real(*F*) =
| 1 | cos(0*2\pi/n)     | cos(0*2\pi/n)     | 1                  | ..  |
| 1 | cos(1*2\pi/n)     | cos(2*2\pi/n)     | cos(3*2\pi/n\)     | ..  |
| 1 | cos(2*2\pi/n)     | cos(4*2\pi/n)     | cos(6*2\pi/n)      | ..  |
| 1 | cos(3*2\pi/n)     | cos(6*2\pi/n)     | cos(9*2\pi/n)      | ..  |
| 1 | ...               | ...               | ...                | ..  |
| 1 | cos([n-1]*2\pi/n) | cos(2[n-1]2\pi/n) | cos(3[n-1]*2\pi/n) | ..  |

If your input is over 1 second then this maps to a sampled cosine function of 1Hz, 2Hz, 3Hz, etc.. *Note*: that these real components alone are no orthogonal. If you take the inner product of two of these columns you will not get /0/.The complex component will look similar, but with the /sine/ functions insteads and you can't drop the imaginary parts b/c of how they come in in the complex conjugate products of the inner products (you need that sin(..)^{2}+cos(..)^{2}=1 on the diagonal

Since each series (irrespective of the /k/ exponent) has length /n^{1/2}/ (remember that the self-inner norms equal /n/), all the columns of *F* can be normalized in one go by dividing the matrix by /n^{1/2}/. The resulting matrix *(1/n^{1/2})F* is now even better b/c it's orthonormal/unitary. Therefore its inverse is just its Hermitian transpose.

 - [(1/n^{1/2})F]^{-1} = [(1/n^{1/2})F]^{*}
 - F^{-1} = (1/n)F^{*}
 In *F^{-1}F* the diagonal elements will be the column magnitudes, which have been normalized to /1/, and the off diagonal elements will be inner products of orthogonal vectors and therefore /0/ - so the product will give us the identity matrix *I*

* Frequency Space?
We constructed a very convenient basis that's easily invertible and independent of the input and we can now easily move to the basis and back but it's not exactly what one would imagine as "frequency space" and a few things are unresolved

** How does a sinusoidal look in this basis?
Since each column is a mixture of real and imaginary samples of sine/cosine functions we at first glance don't have a straight correspondance between frequencies and the column number.\\
Reusing the trigonometric identities: 
 - sin(-\theta) = -sin(\theta)
 - cos(-\theta) = cos(\theta)
We can carefully pair Euler functions to get back bare sine/cosine functions with no imaginary parts
 - [e^{i\theta} + e^{-i\theta}]/2 = [cos(\theta) + isin(\theta) + cos(-\theta) + isin(-\theta)]/2 = *cos(\theta)*
 - [e^{i\theta} - e^{-i\theta}]/2i = [cos(\theta) + isin(\theta) - cos(-\theta) - isin(-\theta)]/2i = *sin(\theta)*
If we put in /-2\pi\phi/n/ for /\theta/  we back our familiar roots of unity:
 - [\xi^{-\phi} + \xi^{\phi}]/2 = cos(2\pi\phi/n)
 - n*[\xi^{-\phi} + \xi^{\phi}]/2 = cos(2\pi\phi)
 - [\xi^{-\phi} - \xi^{\phi}]/2i = sin(2\pi\phi/n)
 - n*[\xi^{-\phi} - \xi^{\phi}]/2i = sin(2\pi\phi)
#+BEGIN_QUOTE
*Note*: b/c these are cyclical functions:
 - sin(-\theta) = sin(2\pi - \theta)
 - cos(-\theta) = cos(2\pi - \theta)
 - e^{-i\theta} = e^{i(2\pi-\theta)}
 - \xi^-k = \xi^{n-k}
#+END_QUOTE
With some rearranging we can rewrite these as:
 - cos(2\pi\phi) = n*[\xi^{n-\phi} + \xi^{\phi}]/2
 - sin(2\pi\phi) = n*[\xi^{n-\phi} - \xi^{\phi}]/2i
and if /\phi/ is equal to a /k/ value, then this will be equal to basis vectors in our fourier matrix.

For instance if /\phi/ = 3 we can pick an /x/
- x = [ 0 0 n/2 0 0 .. 0 0 n/2 0 0]_{1,n}
So that:
- cos(2\pi * 3) = F^{*}x^{T}

Similarly with the sine function, except with an extra minus sign:
- y = [ 0 0 -n/2i 0 0 .. 0 0 n/2i 0 0]_{1,n}
So that:
- sin(2\pi * 3) = F^{*}y^{T}

Both of these matrix-vector products are always producing real values and complex coordinate value has a real and imaginary part, so it stands for both a have a sine and a cosine of it's corresponding basis vector frequency.

Furthermore there is a consistent symmetry between the n^{th} and (n-\phi)^th coordinate. This symmetry in both real and imaginary terms the first n/2 terms tell us everything about the oscillating components of our input. You can completely disregard the second n/2 terms.
#+BEGIN_QUOTE
*Note* This doesn't represent a loss of information. The input had *n* real values the output has *n/2* complex coordinates (each made of 2 values).\\
*Note* If the input is complex then this symmertry won't hold. But in most applications a complex input is not meaningful
#+END_QUOTE

** How does a phase shift look in this basis?
Notice how at each coordinate we had both real and imaginary values representing cosines and sines of the same frequency. Thanks to *harmonic addition* we can add sines and cosines of the same frequency to make phase shifted sinusoids
- \alpha sin(f*t) + \beta cos(f*t) = \gamma sin(f*t+\theta)
- \gamma = \radic[\alpha^{2}+\beta^{2}]
- \theta = atan(\beta/\alpha)

Since matrix products are linear, we can just take our coordinate vector /v/ and split it, feed it through our inverse fourier matrix/linear system, get the sine and cosine function samples, and then add them back:
- v = \real(v) + \image(v)
- vF^{*} = \real(v)F^{*} + \image(v)F^{*} = \alpha sin(...) + \beta cos(...)
Now the form matches the harmonic addition formula and it should be very clear that the real/imaginary parts of the coordinate not only correspond to sine/cosine functions of the same frequency but that they in conjuction represent a phase shifted sine function.

Furthermore, no matter what \theta phase shift you give on the input, the corresponding coordinate will maintain a constant norm/length b/c  \radic[\alpha^{2}+\beta^{2}] will always be equal to the sine wave's amplitude \gamma. If you take the norm of *Fx* (or more simply just do *Fx(Fx)^{*}*) you will get the amplitude at each frequency component (at the expense of loosing all phase information) and you can then draw a spectrogram

** How does a frequency who's period isn't a whole fraction of the sample rate come out in this basis?

All the previous math had assumed for simplicity that the input was at a frequency that matches one of the basis vectors - that it in effect produced one coordinate point. But how about if /\phi/ is not equal to a /k/ value?

