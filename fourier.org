#+TITLE: MoreLinear \\ Fourier
#+DESCRIPTION: linear algebra methods in Clojure

#+INCLUDE: "../web/config.org"

* Goal
Using the multiplicative properties of complex numbers we construct an easily-invertible orthogonal basis in which periodic components of our input become apparant. This basis allows us to carry out a class of operations that are numerically intensive in the standard basis but much quicker in the frequency-basis.

* Complex numbers
Complex numbers are of the form *a+ib* where *i= \radic(-1)* and *a* and *b* are known as the real and imaginary components of the complex number. The pair (a,b) are often referred to as a point on the complex plane. However the implicit analogy to 2D coordinates is a bit probematic

To envision a complex plane we treat the /real/ component *a* acting as the /x/ and the /imaginary/ component *b* acting as the /y/.
Like 2D coordinates, we can add/translate them and they can be scaled
 - (a+ib) + (c+id) = (a+c) + i(bd)
 - c * (a+ib) = ca +icb 

However unlike 2D [x,y] coordinates, complex numbers can be multiplied:
 - (a+ib) * (c+id) = (ac-bd) + i(ad+bc)

Normally if we are given 2 coordinates [x_{0},y_{0}] and [x_{1},y_{1}] we never ask ourselves to multiple them because multiplication between two points is simply no a defined operation. However in the complex-number case this operation comes naturally and it generates a new complex number

#+BEGIN_QUOTE
Note: You can do a dot and cross product between two [x,y] coordinates but:
 - [x_{0},y_{0}] \bullet [x_{1},y_{1}] \to /scalar/
 - [x_{0},y_{0}] \times [x_{1},y_{1}] \to /requires an extra =z= dimension/
#+END_QUOTE
 
So points in the complex plane have this extra "feature" so it's important to not stretch the 2D coordinate interpretation too far. As far as I can tell there is no easy way to visualize what the result of a multiplication looks like in the general case.

* Complex Conjugates
Just like points and vectors, complex numbers have a "length" or magnitude
 - ||a+ib|| = \radic(a^{2}+b^{2}}
And just like for points and vectors, this norm is a real scalar value

The square of the norm is ofcourse just /a^{2}+b^{2}/ and this often comes up in things like the inner product.  For instance given a vector [x y] we can take its inner product with itself [x y][x y]^{T} to get its norm-squared. In the trivial =1x1= case this will be [z][z]^{T} which is z^{2}.

In the complex case where /z/ is a complex number /a+ib/ then we'd get /z*z^{T}=z^{2}=(a+ib)^{2}=a^{2}+i2ab-b^{2}/ which is non-real and doesn't match our magnitude-squared. So using the transpose no longer works. But this norm-squared is still easy to express as a product
 - a^{2}+b^{2} = (a+ib)(a-ib)
With /a-ib/ known as the /complex conjugate/ of /a+ib/

And what's convenient is that this actually still works for real values. Since a real value /a/ can be written as the complex number a+i*0 we can write its complex conjugate as a-i*0 and see that it's norm is
 - a^2+0^2 = (a+i0)(a-i0) = a^2
Which is just the absolute value of /a/ squared

So instead of doing a transpose we need to introduce a new transposition called the Hermitian transpose
 - [a+ib c+id]^{*} = [a-ib c-d]^{T}
The Hermitian transpose is denoted with a star. It simply take a normal transpose and replaces the terms with their complex conjugates.

 When the vector values are all real it will behave exactly like a transpose and when the values are complex, it will take their conjugates and give us scalar/real values of each complex values' magnitude

* The Unit Circle
Notice that when the /norm-squared/ is equal to /1/ the magnitude is also equal to /1/. All these special points who's length is equal to /1/ lie on what's called the /unit cirle/. If we start with two complex numbers /a+ib/ and /c+id/ then
 - (a^2+b^2)^{1/2} = 1 /and therefore/ a^2+b^2 = 1
 - (c^2+d^2)^{1/2} = 1 /and therefore/ c^2+d^2 = 1
As we saw, their product is the point /(ac-bd) + i(ad+bc)/ and its length is also equal to 1. Proof:
 - [ (ac-bd)^{2} + (ad+bc)^{2} ]^(1/2)
 - [ a^{2}c^{2}-2acbd+b^{2}d^{2}+a^{2}d^{2}+2adbc+b^{2}c^{2} ]^{1/2}
 - [ a^{2}c^{2}+b^{2}b^{2}+a^{2}d^{d}+b^{2}c^{2} ]^{1/2}
 - [ c^2(a^2+b^2) + d^2(a^2+b^2) ]^(1/2)
Through some subsitution we see that the length of the product is also equal to /1/

This tells us that while the general complex product is hard to think about, we know that least the product of any two points on the unit circle gives us a new point on the unit circle

* Euler's Formula

Combining the definition of /sine/ and /cosine/ with pythagoras' theorem (for the case where the hypotenous is equal to one) gives us the trigonometric identity:
 - cos^{2}(\theta)+sin^{2}(\theta) = 1

And then Euler's formula tells us that: *(TODO: Exapand on this)*
 - e^{i\theta}=cos(\theta)+isin(\theta)

Combining these two statements allows us to safely say that the magnitude of the complex value /e^{i\theta}/ is always equal to /1/. In fact, every point on the complex-plane unit circle corresponds to a euler exponent. Now using what we know of the product of points on the unit circle we can say that
 -  || e^{i\theta} * e^{i\omega} || = 1
You can also just add the exponents  /e^{i(\theta+\omega)}/ and reusing the trigonometric identity.

Now notice that if /\omega=2\pi-\theta/ that:
 - e^{i(\theta+\omega)} = e^{i2\pi} = cos(2\pi) + isin(2\pi) = 1 + i0 = 1

Also note that complex conjugate of 
 - e^{i\theta}=cos(\theta)+isin(\theta)
is
 - cos(\theta)-isin(\theta)
Using the trigonometric properties (which are self evident if you think of the graph and even/odd functions)
 - sin(-\theta)=-sin(\theta)
 - cos(-\theta)= cos(\theta)
We can rewrite the conjugate as
 - cos(-\theta) + isin{-\theta) = e^{-\theta}

* The roots of unity

Furthermore if /\alpha=2\pi/n/ then:
 - [e^{i\alpha}]^{n} = e^{in\alpha} = e^{i2\pi} = 1
This tells us that taking the n^{th} root of /1/ has a complex numbers solution! (in addition to the trivial solution of /1/)
 - 1^{1/n} = e^{i2\pi/n}
The typical notation here is to say 
 - \xi = e^{-i2\pi/n}
 - \xi^{n}=1
This \xi is called the *n^{th} root of unity*
By extension we can also see that:
 - \xi^{n+j} = \xi^{n}\xi^{j} = \xi^{j}
 - \xi^{nk} = [\xi^{n}]^{k} = [e^{-i2\pi}]^k = 1^k = 1

* Fourier series

The Fourier series is a very special sum of the exponents of an n^{th} root of unity *\xi*
 -  1 + \xi^k + \xi^{2k} + ... + \xi^{(n-2)k} + \xi^{(n-1)k}
Here the /k/ can be any integer value, but the sum will always maintain the property that if it's multiplied by /\xi^k/ we get the same sequence back. The last term goes to /\xi^{nk} = 1/ and the remaining terms in effect shift places giving us:
 -  \xi^k + \xi^{2k} + \xi^{3k} + ... + \xi^{(n-1)k} + 1
So we can write
 - \xi^k * /fourier-series/ = /fourier-series/
Therefore
 - \xi^k * /fourier-series/ - /fourier-series/ = 0
 - /fourier-series/ * (\xi^k - 1) = 0
And therefore.. 
 - /fourier-series/  = 0
.. or to write it out again in full form - for all integer values of /k/
 -  1+\xi^k+\xi^{2k}+...+\xi^{(n-2)k}+\xi^{(n-1)k} = 0

* Fourier Vectors

It turns out that Fourier series, when places in a vector with different values of /k/, form mutually orthogonal vectors - here I choose /r/ and /s/ for /k/ and carry out the Hermitian inner product:
 - [ 1 \xi^r \xi^{2r} ... \xi^{(n-1)r} ] [ 1 \xi^s \xi^{2s} ... \xi^{(n-1)s} ]^{*}
 - [ 1 \xi^r \xi^{2r} ... \xi^{(n-1)r} ] [ 1 \xi^{-s} \xi^{-2s} ... \xi^{-(n-1)s} ]^{T}
 - 1*1 + \xi^r*\xi^-s + \xi^{2r}*\xi^{-2s} + ... + \xi^{(n-2)r}\xi^{-(n-2)s}} + \xi^{(n-1)r}\xi^{-(n-1)s}
 - 1 + \xi^{r-s} + \xi^{2(r-s)} + ... + \xi^{(n-2)(r-s)} + \xi^{(n-1)(r-s)}
We can factor out the (r-s) exponent
 - (1^{1/(r-s)} + \xi^{1} + \xi^{2} + ... + \xi^{n-2} + \xi^{n-1})^{r-s}
 - (1 + \xi^{1} + \xi^{2} + ... + \xi^{n-2} + \xi^{n-1})^{r-s}
And notice that the sum under the exponent is the just fourier series with /k=1/ and therefore equal to /0/. So the inner product of two Fourier series is always equal to zero *except* when /r=s/:
 - [ 1 \xi^k \xi^{2k} ... \xi^{(n-1)k} ]  [ 1 \xi^k \xi^{2k} ... \xi^{(n-1)k} ]^{*}
 - 1*1 + \xi^{k}\xi^{-k} + \xi^{2k}\xi^{-2k} + ... + \xi^{(n-1)k} \xi^{-(n-1)k}
 - 1*1 + \xi^{0} + \xi^{0} + ... + \xi^{0}
 - 1 + 1 + 1 + ... + 1 = n
You could also just view the /r=s/ case as the sum of the 2-norms of the roots of unity. There are /n/ terms and each one is necessarily of length /1/. Either way, the answer will always be /n/ no matter what /k/ you choose
and therefore the 2-norm/length all all fourier vectors is /n^{1/2}/

It's important to note that if you drop all the complex terms none of this works! You can't construct mutually orthogonal vectors out of purely real sine/cosine waves

* Fourier Matrix

We've just shown that the fourier vectors are orthogonal as long as the /k/'s are different. So the next step is self evident. We just choose /k/ to equal to 1 through /n/ and slap them together into a fourier matrix *F* and get ourselves an orthogonal basis:
| 1 | 1         | 1         | 1     | ..  |
| 1 | \xi       | \xi^2     | \xi^3 | ..  |
| 1 | \xi^2     | \xi^4     | \xi^6 | ..  |
| 1 | \xi^3     | \xi^6     | \xi^9 | ..  |
| 1 | ...       | ...       | ...   | ..  |
| 1 | \xi^{n-1} | \xi^{n-2} | ...   | \xi |

Looking at the real components of the columns - there is one constant component (the first column) and then /n/ samples of the /cosine/ function over one oscillation for the second column, /n/ samples over 2 periods for the 3rd column, /n/ samples over 3 periods.. etc.  If your input is over 1 second then this maps to a cosine function of 1Hz, 2Hz, 3Hz, etc.. The complex component will look similar, but with the /sine/ function *Note*: that the real components alone are no orthogonal. You can't drop the imaginary parts b/c of how they come in in the complex conjugate products of the inner products

Since each series (irrespective of the /k/ exponent) has length /n^{1/2}/ (remember that the self-inner norms equal /n/), all the columns of *F* can be normalized in one go by dividing the matrix by /n^{1/2}/. The resulting matrix *(1/n^{1/2})F* is now ever better b/c it's orthonormal/unitary. Therefore its inverse is just its Hermitian transpose.

 - [(1/n^{1/2})F]^{-1} = [(1/n^{1/2})F]^{*}
 - F^{-1} = (1/n)F^{*}
Looking at /F*F^{-1}/, the diagonal elements will be the normalized vector norms of the fourier vectors (/1/'s), and the off-diagonal elements are all inner products of orthogonal fourier vectors (/0/'s)

* Frequency Space?

We constructed a very convenient basis that's easily invertible and independent of the input and we can now easily move to the basis and back but it's not exactly what one would imagine as "frequency space" and a few things are unresolved

** How does a sinusoidal look in this basis?
Reusing the trigonometric identities: 
 - sin(-\theta) = -sin(\theta)
 - cos(-\theta) = cos(\theta)
We can carefully pair Euler functions to get back sinusoids
 - *[e^{i\theta} + e^{-i\theta}]/2* = [cos(\theta) + isin(\theta) + cos(-\theta) + isin(-\theta)]/2 = *cos(\theta)*
 - *[e^{i\theta} - e^{-i\theta}]/2i* = [cos(\theta) + isin(\theta) - cos(-\theta) - isin(-\theta)]/2i = *sin(\theta)*
If we put in /-2\pi\phi/n/ for /\theta/  we back our familiar roots of unity:
 - cos(2\pi\phi/n) = [\xi^{-\phi} + \xi^{\phi}]/2
 - cos(2\pi\phi) = n*[\xi^{-\phi} + \xi^{\phi}]/2
 - sin(2\pi\phi/n) = [\xi^{-\phi} - \xi^{\phi}]/2i
 - sin(2\pi\phi) = n*[\xi^{-\phi} - \xi^{\phi}]/2i
This is a bit goofy b/c we haven't had negative /k/ values so far. Fortunately b/c these are cyclical functions:
 - sin(-\theta) = sin(2\pi - \theta)
 - cos(-\theta) = cos(2\pi - \theta)
 - e^{-i\theta} = e^{i(2\pi-\theta)}
 - \xi^-k = \xi^{n-k}
So we can rewrite is as:
 - cos(2\pi\phi) = n*[\xi^{n-\phi} + \xi^{\phi}]/2
 - sin(2\pi\phi) = n*[\xi^{n-\phi} - \xi^{\phi}]/2i
If /\phi/ is equal to a /k/ value, then this will be equal to basis vectors in our fourier matrix.

For instance if \phi = 3 we can pick an /x/
- x = [ 0 0 n/2 0 0 .. 0 0 n/2 0 0]_{1,n}
So that:
- cos(2\pi * 3) = F^{*}x^{T}

Similarly with the sine function, except:
- y = [ 0 0 -n/2i 0 0 .. 0 0 n/2i 0 0]_{1,n}
So that:
- sin(2\pi * 3) = F^{*}y^{T}

Note two things about these Fourier coordinate vectors:
- the real components are generating /cosine/ functions
- the /n^th/ and /n-\phi/ components are equal
- the imaginary components are generating /sine/ function
- the /n^th/ and /n-\phi/ components are negatives of each other

Because of this symmetry the first n/2 terms tell us everything about oscillating components of our input. 

#+BEGIN_QUOTE
*Note* This doesn't represent a loss of information. The input had *n* real values the output has *n/2* complex coordinates (each made of 2 values)
#+END_QUOTE

** How does a phase shift look in this basis?
Thanks to *harmonic addition* we can add sines and cosines of the same frequency to make phase shifted sinusoids
- \alpha sin(f*t) + \beta cos(f*t) = \gamma sin(f*t+\theta)
- \gamma = \radic[\alpha^{2}+\beta^{2}]
- \theta = atan(\beta/\alpha)

Since each fourier coordinate corresponds to one frequency in effect the real part of the coordinate is the \beta and our the imaginary part is the \alpha in the above formula. So look at just the first /n/2/ coordinates' complex values we can reinterpret these values as a phase shifted sine waves

Furthermore, given a phase shifted sine wave this tells us that the length/norm of the complex value will remain constant b/c  \radic[\alpha^{2}+\beta^{2}] will always be equal to the sine wave's amplitude irrespective of phase shift.

So if you take your whole input and shift it by some any angle \theta all the fourier coordinates' real and imaginary values (the /alpha and /beta) will change a bit but their norms will remain the same. So it you do vv^{*} on a Fourier coordinate vector you will get the magnitude of each frequency component of the input (though in so doing you will lose all phase information)
** How does a frequency who's period isn't a whole fraction of the sample rate come out in this basis?

How about if /\phi/ is not equal to a /k/ value?

